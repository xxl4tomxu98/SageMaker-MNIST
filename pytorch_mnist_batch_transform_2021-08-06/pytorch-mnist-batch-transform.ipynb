{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0db65f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (21.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-21.2.3-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 35.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.1.2\n",
      "    Uninstalling pip-21.1.2:\n",
      "      Successfully uninstalled pip-21.1.2\n",
      "Successfully installed pip-21.2.3\n",
      "Collecting nvidia-ml-py3\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "Building wheels for collected packages: nvidia-ml-py3\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19191 sha256=15f49596b6c3bc6d52bc478b10e9bc265ec2c6733afd81906fb78b1e0cd25755\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/7f/26/a3/33f2079871e2bebb3f53a2b21c3ec64129b8efdd18a6263a52\n",
      "Successfully built nvidia-ml-py3\n",
      "Installing collected packages: nvidia-ml-py3\n",
      "Successfully installed nvidia-ml-py3-7.352.0\n",
      "Found existing installation: torchvision 0.5.0\n",
      "Uninstalling torchvision-0.5.0:\n",
      "  Would remove:\n",
      "    /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision\n",
      "    /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision-0.5.0-py3.6.egg-info\n",
      "Proceed (Y/n)?   Successfully uninstalled torchvision-0.5.0\n",
      "yes: standard output: Broken pipe\n",
      "yes: write error\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.10.0-cp36-cp36m-manylinux1_x86_64.whl (22.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 22.1 MB 30.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch==1.9.0\n",
      "  Downloading torch-1.9.0-cp36-cp36m-manylinux1_x86_64.whl (831.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 831.4 MB 3.3 kB/s  eta 0:00:016��███▎                      | 241.8 MB 94.3 MB/s eta 0:00:07     |██████████████████████████████▌ | 792.0 MB 92.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow>=5.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision) (8.2.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch==1.9.0->torchvision) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch==1.9.0->torchvision) (0.8)\n",
      "Installing collected packages: torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.4.0\n",
      "    Uninstalling torch-1.4.0:\n",
      "      Successfully uninstalled torch-1.4.0\n",
      "Successfully installed torch-1.9.0 torchvision-0.10.0\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!pip install nvidia-ml-py3\n",
    "!yes | pip uninstall torchvision\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04569373",
   "metadata": {
    "papermill": {
     "duration": 0.009489,
     "end_time": "2021-06-03T00:10:10.266437",
     "exception": false,
     "start_time": "2021-06-03T00:10:10.256948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PyTorch Batch Inference\n",
    "In this notebook, we'll examine how to do batch transform task with PyTorch in Amazon SageMaker. \n",
    "\n",
    "First, an image classification model is build on MNIST dataset. Then, we demonstrate batch transform by using SageMaker Python SDK PyTorch framework with different configurations\n",
    "- `data_type=S3Prefix`: uses all objects that match the specified S3 key name prefix for batch inference.\n",
    "- `data_type=ManifestFile`: a manifest file containing a list of object keys that you want to batch inference.\n",
    "- `instance_count>1`: distribute the batch inference dataset to multiple inference instance\n",
    "\n",
    "For batch transform in TensorFlow in Amazon SageMaker, you can follow other Jupyter notebooks [here](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker_batch_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f378606e",
   "metadata": {
    "papermill": {
     "duration": 0.009319,
     "end_time": "2021-06-03T00:10:10.285106",
     "exception": false,
     "start_time": "2021-06-03T00:10:10.275787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup\n",
    "We'll begin with some necessary imports, and get an Amazon SageMaker session to help perform certain tasks, as well as an IAM role with the necessary permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e600183",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T00:10:10.310480Z",
     "iopub.status.busy": "2021-06-03T00:10:10.309977Z",
     "iopub.status.idle": "2021-06-03T00:10:11.972019Z",
     "shell.execute_reply": "2021-06-03T00:10:11.971547Z"
    },
    "papermill": {
     "duration": 1.677667,
     "end_time": "2021-06-03T00:10:11.972131",
     "exception": false,
     "start_time": "2021-06-03T00:10:10.294464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket:\n",
      "sagemaker-us-east-1-804604702169\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from shutil import copyfile\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-pytorch-batch-inference-script\"\n",
    "print(\"Bucket:\\n{}\".format(bucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf210cf1",
   "metadata": {
    "papermill": {
     "duration": 0.009748,
     "end_time": "2021-06-03T00:10:11.992188",
     "exception": false,
     "start_time": "2021-06-03T00:10:11.982440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6a8fcf",
   "metadata": {
    "papermill": {
     "duration": 0.009924,
     "end_time": "2021-06-03T00:10:12.012090",
     "exception": false,
     "start_time": "2021-06-03T00:10:12.002166",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Since the main purpose of this notebook is to demonstrate SageMaker PyTorch batch transform, **we reuse this SageMaker Python SDK [PyTorch example](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/pytorch_mnist) to train a PyTorch model**. It takes around 7 minutes to finish the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1c809b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T00:10:12.038135Z",
     "iopub.status.busy": "2021-06-03T00:10:12.037362Z",
     "iopub.status.idle": "2021-06-03T00:15:42.451109Z",
     "shell.execute_reply": "2021-06-03T00:15:42.449969Z"
    },
    "papermill": {
     "duration": 330.429296,
     "end_time": "2021-06-03T00:15:42.451328",
     "exception": true,
     "start_time": "2021-06-03T00:10:12.022032",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-images-idx3-ubyte.gz\n",
      "Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b99a14fad864f66b1672368d1e3d909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-labels-idx1-ubyte.gz\n",
      "Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071345f6cd0346e1bc240727411a443f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c870cd8204e443898b414ede6732a085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5587577699994fe8a897c4446d907a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-east-1-804604702169/sagemaker/DEMO-pytorch-batch-inference-script\n",
      "2021-08-06 19:20:30 Starting - Starting the training job...\n",
      "2021-08-06 19:20:41 Starting - Launching requested ML instancesProfilerReport-1628277630: InProgress\n",
      "......\n",
      "2021-08-06 19:21:58 Starting - Preparing the instances for training.........\n",
      "2021-08-06 19:23:18 Downloading - Downloading input data......\n",
      "2021-08-06 19:24:24 Training - Downloading the training image..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-08-06 19:24:39,923 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-08-06 19:24:39,925 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-08-06 19:24:39,933 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2021-08-06 19:24:41,009 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2021-08-06 19:24:41,011 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2021-08-06 19:24:41,019 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-08-06 19:25:44,696 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-08-06 19:25:44,699 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-08-06 19:25:44,713 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:25:44,719 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\n",
      "2021-08-06 19:25:59 Training - Training image download completed. Training in progress.\u001b[34m2021-08-06 19:25:52,258 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-08-06 19:25:52,269 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-08-06 19:25:52,279 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-08-06 19:25:52,288 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2021-08-06-19-20-30-135\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-804604702169/pytorch-training-2021-08-06-19-20-30-135/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"backend\":\"gloo\",\"epochs\":1}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-804604702169/pytorch-training-2021-08-06-19-20-30-135/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\"],\"hyperparameters\":{\"backend\":\"gloo\",\"epochs\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-08-06-19-20-30-135\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-804604702169/pytorch-training-2021-08-06-19-20-30-135/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--epochs\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 mnist.py --backend gloo --epochs 1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mDistributed training - True\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 0\u001b[0m\n",
      "\u001b[35m2021-08-06 19:26:01,015 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2021-08-06 19:26:01,460 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2021-08-06 19:26:01,471 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2021-08-06 19:26:01,482 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2021-08-06 19:26:01,490 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[35mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"pytorch-training-2021-08-06-19-20-30-135\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-804604702169/pytorch-training-2021-08-06-19-20-30-135/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"backend\":\"gloo\",\"epochs\":1}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-804604702169/pytorch-training-2021-08-06-19-20-30-135/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\"],\"hyperparameters\":{\"backend\":\"gloo\",\"epochs\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"pytorch-training-2021-08-06-19-20-30-135\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-804604702169/pytorch-training-2021-08-06-19-20-30-135/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--epochs\",\"1\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[35mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[35mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.6 mnist.py --backend gloo --epochs 1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[32m2021-08-06 19:26:02,132 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[32m2021-08-06 19:26:02,634 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m2021-08-06 19:26:02,646 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m2021-08-06 19:26:02,658 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m2021-08-06 19:26:02,667 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[32mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[32m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-3\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"pytorch-training-2021-08-06-19-20-30-135\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-804604702169/pytorch-training-2021-08-06-19-20-30-135/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-3\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[32m}\n",
      "\u001b[0m\n",
      "\u001b[32mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[32mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\"]\u001b[0m\n",
      "\u001b[32mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[32mSM_HPS={\"backend\":\"gloo\",\"epochs\":1}\u001b[0m\n",
      "\u001b[32mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[32mSM_RESOURCE_CONFIG={\"current_host\":\"algo-3\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[32mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[32mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[32mSM_CURRENT_HOST=algo-3\u001b[0m\n",
      "\u001b[32mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[32mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[32mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[32mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[32mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[32mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[32mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[32mSM_MODULE_DIR=s3://sagemaker-us-east-1-804604702169/pytorch-training-2021-08-06-19-20-30-135/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[32mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-3\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\"],\"hyperparameters\":{\"backend\":\"gloo\",\"epochs\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"pytorch-training-2021-08-06-19-20-30-135\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-804604702169/pytorch-training-2021-08-06-19-20-30-135/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-3\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[32mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--epochs\",\"1\"]\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[32mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[32mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[32mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[32mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[32mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[32m/opt/conda/bin/python3.6 mnist.py --backend gloo --epochs 1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35mDistributed training - True\u001b[0m\n",
      "\u001b[35mNumber of gpus available - 0\u001b[0m\n",
      "\u001b[35mInitialized the distributed environment: 'gloo' backend on 3 nodes. Current host rank is 1. Number of gpus: 0\u001b[0m\n",
      "\u001b[35mGet train data loader\u001b[0m\n",
      "\u001b[35mUsing downloaded and verified file: /opt/ml/input/data/training/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[35mExtracting /opt/ml/input/data/training/MNIST/raw/train-images-idx3-ubyte.gz to /opt/ml/input/data/training/MNIST/raw\u001b[0m\n",
      "\u001b[35mUsing downloaded and verified file: /opt/ml/input/data/training/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[35mExtracting /opt/ml/input/data/training/MNIST/raw/train-labels-idx1-ubyte.gz to /opt/ml/input/data/training/MNIST/raw\u001b[0m\n",
      "\u001b[35mUsing downloaded and verified file: /opt/ml/input/data/training/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[35mExtracting /opt/ml/input/data/training/MNIST/raw/t10k-images-idx3-ubyte.gz to /opt/ml/input/data/training/MNIST/raw\u001b[0m\n",
      "\u001b[35mUsing downloaded and verified file: /opt/ml/input/data/training/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[35mExtracting /opt/ml/input/data/training/MNIST/raw/t10k-labels-idx1-ubyte.gz to /opt/ml/input/data/training/MNIST/raw\u001b[0m\n",
      "\u001b[35mProcessing...\u001b[0m\n",
      "\u001b[34mInitialized the distributed environment: 'gloo' backend on 3 nodes. Current host rank is 0. Number of gpus: 0\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mUsing downloaded and verified file: /opt/ml/input/data/training/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mExtracting /opt/ml/input/data/training/MNIST/raw/train-images-idx3-ubyte.gz to /opt/ml/input/data/training/MNIST/raw\u001b[0m\n",
      "\u001b[34mUsing downloaded and verified file: /opt/ml/input/data/training/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mExtracting /opt/ml/input/data/training/MNIST/raw/train-labels-idx1-ubyte.gz to /opt/ml/input/data/training/MNIST/raw\u001b[0m\n",
      "\u001b[34mUsing downloaded and verified file: /opt/ml/input/data/training/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mExtracting /opt/ml/input/data/training/MNIST/raw/t10k-images-idx3-ubyte.gz to /opt/ml/input/data/training/MNIST/raw\u001b[0m\n",
      "\u001b[34mUsing downloaded and verified file: /opt/ml/input/data/training/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mExtracting /opt/ml/input/data/training/MNIST/raw/t10k-labels-idx1-ubyte.gz to /opt/ml/input/data/training/MNIST/raw\u001b[0m\n",
      "\u001b[34mProcessing...\u001b[0m\n",
      "\u001b[34mDone!\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mProcesses 20000/60000 (33%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34m[2021-08-06 19:26:05.246 algo-1:25 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[32mDistributed training - True\u001b[0m\n",
      "\u001b[32mNumber of gpus available - 0\u001b[0m\n",
      "\u001b[32mInitialized the distributed environment: 'gloo' backend on 3 nodes. Current host rank is 2. Number of gpus: 0\u001b[0m\n",
      "\u001b[32mGet train data loader\u001b[0m\n",
      "\u001b[32mUsing downloaded and verified file: /opt/ml/input/data/training/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[32mExtracting /opt/ml/input/data/training/MNIST/raw/train-images-idx3-ubyte.gz to /opt/ml/input/data/training/MNIST/raw\u001b[0m\n",
      "\u001b[32mUsing downloaded and verified file: /opt/ml/input/data/training/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[32mExtracting /opt/ml/input/data/training/MNIST/raw/train-labels-idx1-ubyte.gz to /opt/ml/input/data/training/MNIST/raw\u001b[0m\n",
      "\u001b[32mUsing downloaded and verified file: /opt/ml/input/data/training/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[32mExtracting /opt/ml/input/data/training/MNIST/raw/t10k-images-idx3-ubyte.gz to /opt/ml/input/data/training/MNIST/raw\u001b[0m\n",
      "\u001b[32mUsing downloaded and verified file: /opt/ml/input/data/training/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[32mExtracting /opt/ml/input/data/training/MNIST/raw/t10k-labels-idx1-ubyte.gz to /opt/ml/input/data/training/MNIST/raw\u001b[0m\n",
      "\u001b[32mProcessing...\u001b[0m\n",
      "\u001b[32mDone!\u001b[0m\n",
      "\u001b[32mGet test data loader\u001b[0m\n",
      "\u001b[32mProcesses 20000/60000 (33%) of train data\u001b[0m\n",
      "\u001b[32mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[32m[2021-08-06 19:26:05.317 algo-3:25 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35mDone!\u001b[0m\n",
      "\u001b[35mGet test data loader\u001b[0m\n",
      "\u001b[35mProcesses 20000/60000 (33%) of train data\u001b[0m\n",
      "\u001b[35mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[35m[2021-08-06 19:26:05.300 algo-2:25 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2021-08-06 19:26:05.682 algo-2:25 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[2021-08-06 19:26:05.682 algo-2:25 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2021-08-06 19:26:05.682 algo-2:25 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2021-08-06 19:26:05.683 algo-2:25 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2021-08-06 19:26:05.683 algo-2:25 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[2021-08-06 19:26:05.730 algo-2:25 INFO hook.py:584] name:module.conv1.weight count_params:250\u001b[0m\n",
      "\u001b[35m[2021-08-06 19:26:05.730 algo-2:25 INFO hook.py:584] name:module.conv1.bias count_params:10\u001b[0m\n",
      "\u001b[35m[2021-08-06 19:26:05.730 algo-2:25 INFO hook.py:584] name:module.conv2.weight count_params:5000\u001b[0m\n",
      "\u001b[35m[2021-08-06 19:26:05.730 algo-2:25 INFO hook.py:584] name:module.conv2.bias count_params:20\u001b[0m\n",
      "\u001b[35m[2021-08-06 19:26:05.730 algo-2:25 INFO hook.py:584] name:module.fc1.weight count_params:16000\u001b[0m\n",
      "\u001b[35m[2021-08-06 19:26:05.730 algo-2:25 INFO hook.py:584] name:module.fc1.bias count_params:50\u001b[0m\n",
      "\u001b[35m[2021-08-06 19:26:05.730 algo-2:25 INFO hook.py:584] name:module.fc2.weight count_params:500\u001b[0m\n",
      "\u001b[35m[2021-08-06 19:26:05.730 algo-2:25 INFO hook.py:584] name:module.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[35m[2021-08-06 19:26:05.731 algo-2:25 INFO hook.py:586] Total Trainable Params: 21840\u001b[0m\n",
      "\u001b[35m[2021-08-06 19:26:05.731 algo-2:25 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-08-06 19:26:05.560 algo-1:25 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-08-06 19:26:05.561 algo-1:25 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-08-06 19:26:05.561 algo-1:25 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-08-06 19:26:05.561 algo-1:25 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-08-06 19:26:05.561 algo-1:25 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-08-06 19:26:05.604 algo-1:25 INFO hook.py:584] name:module.conv1.weight count_params:250\u001b[0m\n",
      "\u001b[34m[2021-08-06 19:26:05.604 algo-1:25 INFO hook.py:584] name:module.conv1.bias count_params:10\u001b[0m\n",
      "\u001b[34m[2021-08-06 19:26:05.604 algo-1:25 INFO hook.py:584] name:module.conv2.weight count_params:5000\u001b[0m\n",
      "\u001b[34m[2021-08-06 19:26:05.604 algo-1:25 INFO hook.py:584] name:module.conv2.bias count_params:20\u001b[0m\n",
      "\u001b[34m[2021-08-06 19:26:05.604 algo-1:25 INFO hook.py:584] name:module.fc1.weight count_params:16000\u001b[0m\n",
      "\u001b[34m[2021-08-06 19:26:05.604 algo-1:25 INFO hook.py:584] name:module.fc1.bias count_params:50\u001b[0m\n",
      "\u001b[34m[2021-08-06 19:26:05.604 algo-1:25 INFO hook.py:584] name:module.fc2.weight count_params:500\u001b[0m\n",
      "\u001b[34m[2021-08-06 19:26:05.604 algo-1:25 INFO hook.py:584] name:module.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[34m[2021-08-06 19:26:05.604 algo-1:25 INFO hook.py:586] Total Trainable Params: 21840\u001b[0m\n",
      "\u001b[34m[2021-08-06 19:26:05.604 algo-1:25 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[32m[2021-08-06 19:26:05.747 algo-3:25 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[32m[2021-08-06 19:26:05.747 algo-3:25 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[32m[2021-08-06 19:26:05.747 algo-3:25 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[32m[2021-08-06 19:26:05.748 algo-3:25 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[32m[2021-08-06 19:26:05.748 algo-3:25 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[32m[2021-08-06 19:26:05.802 algo-3:25 INFO hook.py:584] name:module.conv1.weight count_params:250\u001b[0m\n",
      "\u001b[32m[2021-08-06 19:26:05.802 algo-3:25 INFO hook.py:584] name:module.conv1.bias count_params:10\u001b[0m\n",
      "\u001b[32m[2021-08-06 19:26:05.802 algo-3:25 INFO hook.py:584] name:module.conv2.weight count_params:5000\u001b[0m\n",
      "\u001b[32m[2021-08-06 19:26:05.802 algo-3:25 INFO hook.py:584] name:module.conv2.bias count_params:20\u001b[0m\n",
      "\u001b[32m[2021-08-06 19:26:05.802 algo-3:25 INFO hook.py:584] name:module.fc1.weight count_params:16000\u001b[0m\n",
      "\u001b[32m[2021-08-06 19:26:05.802 algo-3:25 INFO hook.py:584] name:module.fc1.bias count_params:50\u001b[0m\n",
      "\u001b[32m[2021-08-06 19:26:05.802 algo-3:25 INFO hook.py:584] name:module.fc2.weight count_params:500\u001b[0m\n",
      "\u001b[32m[2021-08-06 19:26:05.803 algo-3:25 INFO hook.py:584] name:module.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[32m[2021-08-06 19:26:05.803 algo-3:25 INFO hook.py:586] Total Trainable Params: 21840\u001b[0m\n",
      "\u001b[32m[2021-08-06 19:26:05.803 algo-3:25 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [6400/20000 (32%)] Loss: 1.944218\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/20000 (32%)] Loss: 1.830737\u001b[0m\n",
      "\u001b[32mTrain Epoch: 1 [6400/20000 (32%)] Loss: 2.076494\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [12800/20000 (64%)] Loss: 0.970919\u001b[0m\n",
      "\u001b[32mTrain Epoch: 1 [12800/20000 (64%)] Loss: 1.229461\u001b[0m\n",
      "\u001b[32mTrain Epoch: 1 [19200/20000 (96%)] Loss: 0.667853\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mTrain Epoch: 1 [12800/20000 (64%)] Loss: 1.132146\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [19200/20000 (96%)] Loss: 0.714048\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [19200/20000 (96%)] Loss: 0.892621\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.4341, Accuracy: 8826/10000 (88%)\n",
      "\u001b[0m\n",
      "\u001b[35mINFO:__main__:Initialized the distributed environment: 'gloo' backend on 3 nodes. Current host rank is 1. Number of gpus: 0\u001b[0m\n",
      "\u001b[35mINFO:__main__:Get train data loader\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\u001b[0m\n",
      "\u001b[35mINFO:__main__:Get test data loader\u001b[0m\n",
      "\u001b[35mDEBUG:__main__:Processes 20000/60000 (33%) of train data\u001b[0m\n",
      "\u001b[35mDEBUG:__main__:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [6400/20000 (32%)] Loss: 1.944218\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [12800/20000 (64%)] Loss: 0.970919\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [19200/20000 (96%)] Loss: 0.714048\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\u001b[0m\n",
      "\u001b[35mINFO:__main__:Test set: Average loss: 0.4341, Accuracy: 8826/10000 (88%)\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35m2021-08-06 19:26:13,653 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.4341, Accuracy: 8826/10000 (88%)\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Initialized the distributed environment: 'gloo' backend on 3 nodes. Current host rank is 0. Number of gpus: 0\u001b[0m\n",
      "\u001b[34mINFO:__main__:Get train data loader\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\u001b[0m\n",
      "\u001b[34mINFO:__main__:Get test data loader\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:Processes 20000/60000 (33%) of train data\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6400/20000 (32%)] Loss: 1.830737\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/20000 (64%)] Loss: 1.132146\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [19200/20000 (96%)] Loss: 0.892621\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.4341, Accuracy: 8826/10000 (88%)\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2021-08-06 19:26:13,630 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[32mTest set: Average loss: 0.4341, Accuracy: 8826/10000 (88%)\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:__main__:Initialized the distributed environment: 'gloo' backend on 3 nodes. Current host rank is 2. Number of gpus: 0\u001b[0m\n",
      "\u001b[32mINFO:__main__:Get train data loader\u001b[0m\n",
      "\u001b[32m/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\u001b[0m\n",
      "\u001b[32mINFO:__main__:Get test data loader\u001b[0m\n",
      "\u001b[32mDEBUG:__main__:Processes 20000/60000 (33%) of train data\u001b[0m\n",
      "\u001b[32mDEBUG:__main__:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[32m/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\u001b[0m\n",
      "\u001b[32mINFO:__main__:Train Epoch: 1 [6400/20000 (32%)] Loss: 2.076494\u001b[0m\n",
      "\u001b[32mINFO:__main__:Train Epoch: 1 [12800/20000 (64%)] Loss: 1.229461\u001b[0m\n",
      "\u001b[32mINFO:__main__:Train Epoch: 1 [19200/20000 (96%)] Loss: 0.667853\u001b[0m\n",
      "\u001b[32m/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\u001b[0m\n",
      "\u001b[32mINFO:__main__:Test set: Average loss: 0.4341, Accuracy: 8826/10000 (88%)\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[32m2021-08-06 19:26:13,636 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-08-06 19:26:27 Uploading - Uploading generated training model\n",
      "2021-08-06 19:26:27 Completed - Training job completed\n",
      "Training seconds: 582\n",
      "Billable seconds: 582\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "local_dir = \"data\"\n",
    "MNIST.mirrors = [\"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/\"]\n",
    "MNIST(\n",
    "    local_dir,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "inputs = sagemaker_session.upload_data(path=local_dir, bucket=bucket, key_prefix=prefix)\n",
    "print(\"input spec (in this case, just an S3 path): {}\".format(inputs))\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"model-script/mnist.py\",\n",
    "    role=role,\n",
    "    framework_version=\"1.8.0\",\n",
    "    py_version=\"py3\",\n",
    "    instance_count=3,\n",
    "    instance_type=\"ml.c5.2xlarge\",\n",
    "    hyperparameters={\n",
    "        \"epochs\": 1,\n",
    "        \"backend\": \"gloo\",\n",
    "    },  # set epochs to a more realistic number for real training\n",
    ")\n",
    "\n",
    "estimator.fit({\"training\": inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f219a661",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Prepare batch inference data\n",
    "\n",
    "First, convert the test data into png image; second, upload to your default S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14efcbda",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t10k-images-idx3-ubyte\t   train-images-idx3-ubyte\r\n",
      "t10k-images-idx3-ubyte.gz  train-images-idx3-ubyte.gz\r\n",
      "t10k-labels-idx1-ubyte\t   train-labels-idx1-ubyte\r\n",
      "t10k-labels-idx1-ubyte.gz  train-labels-idx1-ubyte.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/MNIST/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd3bb6c5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# untar gz => png\n",
    "\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "with gzip.open(os.path.join(local_dir, \"MNIST/raw\", \"t10k-images-idx3-ubyte.gz\"), \"rb\") as f:\n",
    "    images = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a51df0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3026acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample 100 of test images and upload them to S3\n",
    "\n",
    "import random\n",
    "from PIL import Image as im\n",
    "\n",
    "ids = random.sample(range(len(images)), 100)\n",
    "ids = np.array(ids, dtype=np.int)\n",
    "selected_images = images[ids]\n",
    "\n",
    "image_dir = \"data/images\"\n",
    "\n",
    "if not os.path.exists(image_dir):\n",
    "    os.makedirs(image_dir)\n",
    "\n",
    "for i, img in enumerate(selected_images):\n",
    "    pngimg = im.fromarray(img)\n",
    "    pngimg.save(os.path.join(image_dir, f\"{i}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d44265bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['99.png',\n",
       " '51.png',\n",
       " '97.png',\n",
       " '2.png',\n",
       " '5.png',\n",
       " '46.png',\n",
       " '35.png',\n",
       " '41.png',\n",
       " '4.png',\n",
       " '90.png',\n",
       " '58.png',\n",
       " '30.png',\n",
       " '75.png',\n",
       " '68.png',\n",
       " '17.png',\n",
       " '92.png',\n",
       " '54.png',\n",
       " '21.png',\n",
       " '32.png',\n",
       " '38.png',\n",
       " '52.png',\n",
       " '67.png',\n",
       " '94.png',\n",
       " '31.png',\n",
       " '66.png',\n",
       " '37.png',\n",
       " '77.png',\n",
       " '9.png',\n",
       " '60.png',\n",
       " '39.png',\n",
       " '44.png',\n",
       " '73.png',\n",
       " '22.png',\n",
       " '74.png',\n",
       " '96.png',\n",
       " '65.png',\n",
       " '3.png',\n",
       " '25.png',\n",
       " '64.png',\n",
       " '91.png',\n",
       " '16.png',\n",
       " '45.png',\n",
       " '8.png',\n",
       " '72.png',\n",
       " '71.png',\n",
       " '42.png',\n",
       " '63.png',\n",
       " '87.png',\n",
       " '70.png',\n",
       " '61.png',\n",
       " '1.png',\n",
       " '83.png',\n",
       " '49.png',\n",
       " '50.png',\n",
       " '28.png',\n",
       " '79.png',\n",
       " '7.png',\n",
       " '47.png',\n",
       " '62.png',\n",
       " '93.png',\n",
       " '0.png',\n",
       " '56.png',\n",
       " '55.png',\n",
       " '82.png',\n",
       " '89.png',\n",
       " '12.png',\n",
       " '14.png',\n",
       " '98.png',\n",
       " '78.png',\n",
       " '23.png',\n",
       " '85.png',\n",
       " '19.png',\n",
       " '86.png',\n",
       " '80.png',\n",
       " '76.png',\n",
       " '43.png',\n",
       " '10.png',\n",
       " '36.png',\n",
       " '27.png',\n",
       " '33.png',\n",
       " '48.png',\n",
       " '81.png',\n",
       " '20.png',\n",
       " '95.png',\n",
       " '84.png',\n",
       " '69.png',\n",
       " '11.png',\n",
       " '29.png',\n",
       " '59.png',\n",
       " '34.png',\n",
       " '57.png',\n",
       " '15.png',\n",
       " '13.png',\n",
       " '24.png',\n",
       " '18.png',\n",
       " '40.png',\n",
       " '88.png',\n",
       " '26.png',\n",
       " '53.png',\n",
       " '6.png']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f52df3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-east-1-804604702169/batch_transform\n"
     ]
    }
   ],
   "source": [
    "inference_prefix = \"batch_transform\"\n",
    "inference_inputs = sagemaker_session.upload_data(\n",
    "    path=image_dir, bucket=bucket, key_prefix=inference_prefix\n",
    ")\n",
    "print(\"input spec (in this case, just an S3 path): {}\".format(inference_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb2fcb6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Create model transformer\n",
    "Now, we will create a transformer object for handling creating and interacting with Amazon SageMaker transform jobs. We can create the transformer in two ways as shown in the following notebook cells.\n",
    "- use fitted estimator directly\n",
    "- first create PyTorchModel from saved model artefect, then create transformer from PyTorchModel object\n",
    "\n",
    "\n",
    "Here, we implement the `model_fn`, `input_fn`, `predict_fn` and `output_fn` function to override the default [PyTorch inference handler](https://github.com/aws/sagemaker-pytorch-inference-toolkit/blob/master/src/sagemaker_pytorch_serving_container/default_inference_handler.py). \n",
    "\n",
    "It is noted that in `input_fn` function, the inferenced images are encoded as a Python ByteArray. That's why we use `load_from_bytearray` function to load image from `io.BytesIO` then use `PIL.image` to read.\n",
    "\n",
    "```python\n",
    "def model_fn(model_dir):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = torch.nn.DataParallel(Net())\n",
    "    with open(os.path.join(model_dir, 'model.pth'), 'rb') as f:\n",
    "        model.load_state_dict(torch.load(f))\n",
    "    return model.to(device)\n",
    "\n",
    "    \n",
    "def load_from_bytearray(request_body):\n",
    "    image_as_bytes = io.BytesIO(request_body)\n",
    "    image = Image.open(image_as_bytes)\n",
    "    image_tensor = ToTensor()(image).unsqueeze(0)    \n",
    "    return image_tensor\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    # if set content_type as 'image/jpg' or 'applicaiton/x-npy', \n",
    "    # the input is also a python bytearray\n",
    "    if request_content_type == 'application/x-image': \n",
    "        image_tensor = load_from_bytearray(request_body)\n",
    "    else:\n",
    "        print(\"not support this type yet\")\n",
    "        raise ValueError(\"not support this type yet\")\n",
    "    return image_tensor\n",
    "\n",
    "\n",
    "# Perform prediction on the deserialized object, with the loaded model\n",
    "def predict_fn(input_object, model):\n",
    "    output = model.forward(input_object)\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "\n",
    "    return {'predictions':pred.item()}\n",
    "\n",
    "\n",
    "# Serialize the prediction result into the desired response content type\n",
    "def output_fn(predictions, response_content_type):\n",
    "    return json.dumps(predictions)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a7e7842",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use fitted estimator directly\n",
    "transformer = estimator.transformer(instance_count=1, instance_type=\"ml.c5.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af7aabde",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can also create a Transformer object from saved model artefect\n",
    "\n",
    "# get model artefect location by estimator.model_data, or give a S3 key directly\n",
    "model_artefect_s3_location = estimator.model_data  #'s3://BUCKET/PREFIX/model.tar.gz'\n",
    "\n",
    "# create PyTorchModel from saved model artefect\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=model_artefect_s3_location,\n",
    "    role=role,\n",
    "    framework_version=\"1.8.0\",\n",
    "    py_version=\"py3\",\n",
    "    source_dir=\"model-script/\",\n",
    "    entry_point=\"mnist.py\",\n",
    ")\n",
    "\n",
    "# then create transformer from PyTorchModel object\n",
    "transformer = pytorch_model.transformer(instance_count=1, instance_type=\"ml.c5.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a5998",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Batch inference\n",
    "Next, we will inference the sampled 100 MNIST images in a batch manner. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54783fb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### input images directly from S3 location\n",
    "We set `S3DataType=S3Prefix` to uses all objects that match the specified S3 key name prefix for batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b391672e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................\u001b[34m2021-08-06 19:34:18,457 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[34mTorchserve version: 0.3.0\u001b[0m\n",
      "\u001b[34mTS Home: /opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[34mMax heap size: 944 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.6\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mInitial Models: model.mar\u001b[0m\n",
      "\u001b[34mLog dir: /logs\u001b[0m\n",
      "\u001b[34mMetrics dir: /logs\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 4\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[34mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[34mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[34mEnable metrics API: true\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,485 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,501 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 220b2394ecff4b8b98dadfc28f5f552e\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,512 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,542 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,686 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,687 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]44\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,687 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,690 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,696 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,709 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,710 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]45\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,712 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,713 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,712 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,737 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,743 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]46\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,743 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,744 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,744 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,765 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,765 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,774 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,774 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,775 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]47\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,775 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,775 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,775 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,780 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,780 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,785 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:18,787 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:19,425 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:1628278459\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:19,427 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:48.24012756347656|#Level:Host|#hostname:f74e90b8f6d5,timestamp:1628278459\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:19,428 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:7.675785064697266|#Level:Host|#hostname:f74e90b8f6d5,timestamp:1628278459\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:19,429 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:13.7|#Level:Host|#hostname:f74e90b8f6d5,timestamp:1628278459\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:19,429 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:6445.30859375|#Level:Host|#hostname:f74e90b8f6d5,timestamp:1628278459\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:19,430 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:905.94140625|#Level:Host|#hostname:f74e90b8f6d5,timestamp:1628278459\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:19,430 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:15.5|#Level:Host|#hostname:f74e90b8f6d5,timestamp:1628278459\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:19,816 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 928\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:19,816 [INFO ] W-9003-model_1 TS_METRICS - W-9003-model_1.ms:1292|#Level:Host|#hostname:f74e90b8f6d5,timestamp:1628278459\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:19,817 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:102|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:19,907 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1015\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:19,907 [INFO ] W-9002-model_1 TS_METRICS - W-9002-model_1.ms:1383|#Level:Host|#hostname:f74e90b8f6d5,timestamp:1628278459\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:19,907 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:105|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:19,947 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1081\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:19,947 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:1429|#Level:Host|#hostname:f74e90b8f6d5,timestamp:1628278459\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:19,947 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:79|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:20,069 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1181\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:20,070 [INFO ] W-9001-model_1 TS_METRICS - W-9001-model_1.ms:1549|#Level:Host|#hostname:f74e90b8f6d5,timestamp:1628278460\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:20,070 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:102|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,158 [INFO ] pool-1-thread-5 ACCESS_LOG - /169.254.255.130:43700 \"GET /ping HTTP/1.1\" 200 13\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,158 [INFO ] pool-1-thread-5 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,181 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:43708 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,182 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,258 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 10\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,259 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 20\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,259 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,259 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,259 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,262 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:10.65|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:c659b0da-da5f-4b31-a8dc-b752883977a5,timestamp:1628278464\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,344 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,344 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 14\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,345 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,344 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:12.37|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:cf497e08-f9ba-4321-9576-5ebe7cdd2a48,timestamp:1628278464\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,346 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,346 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,511 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 12\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,512 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:11.38|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:027391f7-98a3-4609-89ac-ec2cd127dc48,timestamp:1628278464\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,512 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 14\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,512 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,512 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,516 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:6|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,644 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 8\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,644 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:10.81|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:96143e92-1c17-4439-b9e4-8ab23d49c62d,timestamp:1628278464\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,644 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 12\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,645 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,645 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,645 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:5|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,731 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,732 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,732 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,732 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,732 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,731 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.38|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:f5720701-c0ec-4137-8986-1fa295035ba7,timestamp:1628278464\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,874 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,874 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,875 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,874 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.54|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:20767f13-f1d5-48e5-88c4-ebe803d0d6ba,timestamp:1628278464\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,875 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,875 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,976 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.45|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:c8a1786f-fbd1-4318-a7cc-260ddee4b175,timestamp:1628278464\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,980 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 6\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,981 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 8\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,981 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,981 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:24,981 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,058 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.95|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:c079e2fb-3b3a-48be-bd43-5d47f80af3e9,timestamp:1628278465\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,058 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,059 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 5\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,059 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,059 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,059 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,142 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,142 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.36|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:4ce0eaa3-74a9-4450-a10f-2a47b01189b6,timestamp:1628278465\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,142 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,142 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,142 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,142 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,227 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,227 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,228 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,228 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,229 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,227 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.29|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:5fcd081e-163b-432b-9699-c189d5fb4d0b,timestamp:1628278465\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,335 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,335 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.37|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:e1da6783-7052-4a7b-acc6-8083018fe7d0,timestamp:1628278465\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,335 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,335 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,336 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,336 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,470 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,470 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.53|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:c827cb5d-3939-47dd-aea3-5d9fd312f29f,timestamp:1628278465\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,470 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,470 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,471 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,471 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,590 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.34|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:d8984585-29e6-4353-ae8b-0b0ab5a5c854,timestamp:1628278465\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,590 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,591 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,591 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,591 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,591 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,672 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,672 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.88|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:b5cd881f-52ef-4752-a142-ff62649710cc,timestamp:1628278465\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,672 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,673 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,673 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,673 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,755 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.4|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:84420b93-e60c-4f23-a5fe-dfc067b420b1,timestamp:1628278465\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,755 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,755 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,755 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,756 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,756 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,851 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,851 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.4|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:c7616ede-0f1a-4f2f-a39b-f11c01418a64,timestamp:1628278465\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,851 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,852 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,852 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,852 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,950 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,950 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.56|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:6bdbcc26-155d-4fc5-aa96-45c34756dc95,timestamp:1628278465\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,951 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,951 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,951 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:25,951 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,034 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,034 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.37|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:93396dcb-6161-4bb6-9d8a-e5bb90184217,timestamp:1628278466\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,034 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,035 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,035 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,035 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,118 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,118 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.69|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:dabf7bab-934e-45d1-bfe5-214b836c1f02,timestamp:1628278466\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,118 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,119 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,119 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,119 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,202 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,202 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.36|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:f57eac1b-784b-4f93-a300-91b7f71cf55e,timestamp:1628278466\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,203 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,203 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,203 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,203 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,301 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,301 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.47|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:aa4ff54e-3a2c-45eb-8d1e-3860ddc9b73d,timestamp:1628278466\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,301 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,301 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,302 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,302 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,375 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,375 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.77|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:d3093ef4-ac08-4325-94a6-2f51905fb778,timestamp:1628278466\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,375 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,375 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,375 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,376 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,476 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.88|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:5fde77c4-5259-4b8d-8ee4-7a73b200b93d,timestamp:1628278466\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,476 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,477 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 5\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,477 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,477 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,477 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,561 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,561 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.62|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:3c105496-8343-47e8-854a-74a69140894e,timestamp:1628278466\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,562 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,562 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,562 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,562 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,673 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,673 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.45|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:75297239-045c-4492-ac61-95ad97de91bc,timestamp:1628278466\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,673 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,673 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,674 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,674 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,925 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.46|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:23cdb20a-f162-478e-92a4-a7e7bccb0e1a,timestamp:1628278466\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,926 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,926 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,926 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,926 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:26,926 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,003 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,004 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,003 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.31|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:bfa9bee8-e7e4-475c-ae3c-6577f7434a25,timestamp:1628278467\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,004 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,004 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,005 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,087 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,087 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,088 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,087 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.4|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:a31b586c-3d38-44ed-9122-3f203e6980b5,timestamp:1628278467\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,088 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,088 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,167 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.76|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:f4fde480-9804-4bc0-8e32-c7e3fae5ce62,timestamp:1628278467\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,167 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,168 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,168 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,168 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,168 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,254 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,254 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.37|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:af76a150-24a1-4cbd-bd2e-25f0898f708a,timestamp:1628278467\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,254 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,254 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,254 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,254 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,331 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,331 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.31|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:23d46dc7-cbb3-41d4-997b-29f507cd8486,timestamp:1628278467\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,331 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,331 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,331 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,331 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,474 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,474 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.28|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:6f64a5dc-d235-4fb1-8446-7e9ecf2d0050,timestamp:1628278467\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,475 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,475 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,475 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,475 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,565 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,565 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,566 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,566 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,566 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,565 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.47|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:b03b1c24-f8fd-4e17-8632-2c979688c10f,timestamp:1628278467\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,645 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.36|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:420697b8-0471-4380-a1d4-6140d3d75fd6,timestamp:1628278467\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,647 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,647 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,647 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,647 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,647 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,739 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.43|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:2741a2f3-0184-4c06-a797-786d93f2166f,timestamp:1628278467\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,739 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,739 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,739 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,740 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,740 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,899 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.38|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:51dec811-0c33-4a6e-aded-265cd0fed7fc,timestamp:1628278467\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,899 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,900 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,900 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,900 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:27,900 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,010 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,010 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.29|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:a58a6ef6-a2f2-4b55-b630-7be733b22cd0,timestamp:1628278468\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,010 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,010 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,010 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,011 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,116 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,116 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.26|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:247e3f5b-5481-4f8d-9151-2c1566242771,timestamp:1628278468\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,116 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,116 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,116 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,116 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,203 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.71|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:dc077eee-5afd-4a8b-b5da-d58a439dda5a,timestamp:1628278468\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,203 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,203 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,203 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,203 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,203 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,284 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,284 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.67|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:ade94f0c-c75d-4c4e-bd0a-ea9959d8bde2,timestamp:1628278468\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,285 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,285 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,285 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,285 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,392 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,392 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.29|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:4ea17889-2588-4cd6-bf20-ed1d5a13bd52,timestamp:1628278468\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,393 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,393 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,393 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,393 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,538 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,538 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.35|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:7eef7cc6-8826-4b7a-b9ba-0edacb3171bd,timestamp:1628278468\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,538 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,538 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,538 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,539 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,642 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.29|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:06b7b3bc-4e23-47ce-8463-746e4a7abe33,timestamp:1628278468\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,642 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,642 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,642 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,642 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,642 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,743 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,743 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.26|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:a356d56d-851d-440b-a56f-4988d4d38a11,timestamp:1628278468\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,743 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,743 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,743 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,743 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,824 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,824 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.44|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:5219847e-239d-4be5-b47c-52e459c2a07a,timestamp:1628278468\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,825 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,825 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,825 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,825 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,927 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,927 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.3|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:9fbf124e-fc98-447b-a228-09cd2eadc786,timestamp:1628278468\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,927 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,927 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,927 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:28,927 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,032 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,032 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.39|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:45c614ca-cb06-448d-9a61-7cf430d81bf8,timestamp:1628278469\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,033 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,033 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,033 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,033 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,121 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,122 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,121 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.69|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:40ab5569-20c7-4f80-b8c1-264baa6faae5,timestamp:1628278469\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,122 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,122 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,122 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2021-08-06T19:34:24.188:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,215 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,215 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.35|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:719d1561-58b1-4ba3-9700-5e69985a8066,timestamp:1628278469\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,215 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,215 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,215 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,215 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,301 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,301 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.33|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:a07384f0-7bc8-41f2-b681-58804c84a883,timestamp:1628278469\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,301 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,301 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,301 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,301 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,389 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,215 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,215 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.35|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:719d1561-58b1-4ba3-9700-5e69985a8066,timestamp:1628278469\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,215 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,215 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,215 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,215 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,301 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,301 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.33|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:a07384f0-7bc8-41f2-b681-58804c84a883,timestamp:1628278469\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,301 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,301 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,301 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,301 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,389 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,389 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.5|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:d54bb7d8-7d0c-42a4-bfa4-8578beea833f,timestamp:1628278469\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,390 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,390 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,390 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,390 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,528 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,528 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.59|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:68e91084-ae6e-471f-9210-21d38a5c2214,timestamp:1628278469\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,529 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,529 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,529 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,529 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,666 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,666 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.52|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:b24e00d5-a853-465c-bf75-4d8e88967890,timestamp:1628278469\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,666 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,666 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,666 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,667 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,850 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,851 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,851 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,851 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.76|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:522d1905-cd7b-4297-8de2-30f539efecb5,timestamp:1628278469\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,851 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,851 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,931 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,931 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.5|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:126cbfab-64d2-428e-a9fa-8453777e42fb,timestamp:1628278469\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,931 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,931 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,931 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:29,932 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,010 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,010 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.4|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:352b89cd-d407-4d43-bc97-126f17f5ff1e,timestamp:1628278470\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,011 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,011 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,011 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,011 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,389 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.5|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:d54bb7d8-7d0c-42a4-bfa4-8578beea833f,timestamp:1628278469\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,390 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,390 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,390 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,390 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,528 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,528 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.59|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:68e91084-ae6e-471f-9210-21d38a5c2214,timestamp:1628278469\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,529 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,529 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,529 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,529 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,666 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,666 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.52|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:b24e00d5-a853-465c-bf75-4d8e88967890,timestamp:1628278469\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,666 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,666 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,666 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,667 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,850 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,851 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,851 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,851 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.76|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:522d1905-cd7b-4297-8de2-30f539efecb5,timestamp:1628278469\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,851 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,851 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,931 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,931 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.5|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:126cbfab-64d2-428e-a9fa-8453777e42fb,timestamp:1628278469\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,931 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,931 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,931 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:29,932 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,010 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,010 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.4|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:352b89cd-d407-4d43-bc97-126f17f5ff1e,timestamp:1628278470\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,011 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,011 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,011 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,011 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,169 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,169 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,169 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.39|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:57a3d728-d325-443d-a540-18a0a7feee00,timestamp:1628278470\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,169 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,169 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,169 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,169 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,254 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,254 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.42|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:d1ac0e8f-dc2d-4691-a768-035715011be2,timestamp:1628278470\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,254 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,254 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,255 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,255 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,411 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,411 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.42|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:491cf4f6-26dc-48ea-8e1a-af7c072606f7,timestamp:1628278470\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,411 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,411 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,411 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,411 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,523 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 8\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,523 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:7.3|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:79e7ae05-0b26-4b6a-a55e-4824b18fefa7,timestamp:1628278470\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,524 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 9\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,524 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,524 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,524 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,615 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,615 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,615 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.28|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:121bbced-0649-49f5-a95f-ac0d0113baf6,timestamp:1628278470\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,615 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,616 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,616 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,730 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.49|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:5158d1fe-cd1c-483d-b354-398ac475b942,timestamp:1628278470\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,731 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,731 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,731 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,731 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,731 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,810 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,811 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.62|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:06f331a4-6c18-47e5-9fda-c6d9a8f28395,timestamp:1628278470\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,811 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,811 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,811 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,811 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,893 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,894 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,894 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,894 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.37|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:02b50b93-88a8-4369-845b-17ff0af7abc9,timestamp:1628278470\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,894 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:30,894 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,024 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,024 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,024 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,024 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,025 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,025 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.76|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:95d900f9-363f-4ba9-b6d0-e4d3caa62abd,timestamp:1628278471\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,111 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,111 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.34|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:041882f3-95ad-444f-a555-d3699c63b316,timestamp:1628278471\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,111 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,111 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,111 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,169 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.39|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:57a3d728-d325-443d-a540-18a0a7feee00,timestamp:1628278470\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,169 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,169 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,169 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,169 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,254 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,254 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.42|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:d1ac0e8f-dc2d-4691-a768-035715011be2,timestamp:1628278470\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,254 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,254 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,255 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,255 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,411 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,411 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.42|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:491cf4f6-26dc-48ea-8e1a-af7c072606f7,timestamp:1628278470\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,411 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,411 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,411 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,411 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,523 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 8\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,523 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:7.3|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:79e7ae05-0b26-4b6a-a55e-4824b18fefa7,timestamp:1628278470\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,524 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 9\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,524 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,524 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,524 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,615 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,615 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,615 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.28|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:121bbced-0649-49f5-a95f-ac0d0113baf6,timestamp:1628278470\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,615 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,616 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,616 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,730 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.49|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:5158d1fe-cd1c-483d-b354-398ac475b942,timestamp:1628278470\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,731 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,731 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,731 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,731 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,731 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,810 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,811 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.62|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:06f331a4-6c18-47e5-9fda-c6d9a8f28395,timestamp:1628278470\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,811 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,811 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,811 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,811 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,893 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,894 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,894 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,894 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.37|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:02b50b93-88a8-4369-845b-17ff0af7abc9,timestamp:1628278470\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,894 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:30,894 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,024 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,024 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,024 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,024 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,025 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,025 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.76|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:95d900f9-363f-4ba9-b6d0-e4d3caa62abd,timestamp:1628278471\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,111 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,111 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.34|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:041882f3-95ad-444f-a555-d3699c63b316,timestamp:1628278471\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,111 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,111 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,111 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,111 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,111 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,220 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,220 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.54|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:923b551d-eceb-4a61-a843-ca98f23b76d3,timestamp:1628278471\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,220 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,220 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,220 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,220 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,302 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,302 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.37|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:001ed317-fe0c-4bf5-b232-85f3e22c6249,timestamp:1628278471\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,302 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,303 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,303 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,303 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,406 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,406 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.35|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:675de05d-6da5-41c7-af46-875623858d0d,timestamp:1628278471\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,406 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,406 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,406 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,406 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,524 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,524 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.28|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:e00057ea-e1e5-41fe-b78f-766fc692e822,timestamp:1628278471\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,524 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,524 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,524 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,524 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,666 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.37|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:59fc5800-80a4-4e2e-9f58-aeaab56f0560,timestamp:1628278471\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,666 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,667 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,667 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,667 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,667 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,771 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,772 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,771 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.28|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:08369119-ec94-45cf-8a8a-6309ecaeb2e6,timestamp:1628278471\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,772 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,772 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,772 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,873 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,220 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,220 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.54|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:923b551d-eceb-4a61-a843-ca98f23b76d3,timestamp:1628278471\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,220 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,220 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,220 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,220 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,302 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,302 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.37|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:001ed317-fe0c-4bf5-b232-85f3e22c6249,timestamp:1628278471\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,302 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,303 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,303 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,303 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,406 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,406 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.35|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:675de05d-6da5-41c7-af46-875623858d0d,timestamp:1628278471\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,406 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,406 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,406 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,406 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,524 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,524 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.28|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:e00057ea-e1e5-41fe-b78f-766fc692e822,timestamp:1628278471\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,524 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,524 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,524 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,524 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,666 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.37|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:59fc5800-80a4-4e2e-9f58-aeaab56f0560,timestamp:1628278471\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,666 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,667 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,667 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,667 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,667 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,771 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,772 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,771 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.28|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:08369119-ec94-45cf-8a8a-6309ecaeb2e6,timestamp:1628278471\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,772 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,772 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,772 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,873 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,873 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.31|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:b4985b9b-8823-4ca5-b8e8-4d2a4c2996d0,timestamp:1628278471\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,873 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,873 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,874 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,874 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,965 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.72|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:19bfc78e-79f5-473d-8e23-e1ef3e591a22,timestamp:1628278471\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,965 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,965 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,965 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,966 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:31,966 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,123 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,123 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,123 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,123 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,123 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,123 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.53|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:8bca9dab-3a91-4c2e-acd5-6df9d065fb1e,timestamp:1628278472\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,873 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.31|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:b4985b9b-8823-4ca5-b8e8-4d2a4c2996d0,timestamp:1628278471\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,873 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,873 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,874 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,874 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,965 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.72|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:19bfc78e-79f5-473d-8e23-e1ef3e591a22,timestamp:1628278471\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,965 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,965 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,965 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,966 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:31,966 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,123 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,123 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,123 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,123 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,123 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,123 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.53|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:8bca9dab-3a91-4c2e-acd5-6df9d065fb1e,timestamp:1628278472\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,215 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,215 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.33|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:b7d0b0d7-2ddc-4cc5-99b4-7fe3005c7f29,timestamp:1628278472\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,215 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,215 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,215 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,215 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,295 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,295 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.28|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:3074829c-66bd-4e06-963c-e8adc27a5773,timestamp:1628278472\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,295 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,215 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,215 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.33|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:b7d0b0d7-2ddc-4cc5-99b4-7fe3005c7f29,timestamp:1628278472\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,215 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,215 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,215 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,215 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,295 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,295 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.28|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:3074829c-66bd-4e06-963c-e8adc27a5773,timestamp:1628278472\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,295 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,295 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,295 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,295 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,390 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,390 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.83|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:25292394-a306-448e-8e2d-303ebe3525ec,timestamp:1628278472\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,390 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,390 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,390 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,390 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,467 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,467 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.65|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:5d08df58-cb09-4a7a-bdd0-2da648520a48,timestamp:1628278472\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,467 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,467 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,467 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,467 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,553 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,553 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.31|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:2fca53ad-e0b2-4e76-b9d8-08d161c4254d,timestamp:1628278472\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,553 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,553 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,554 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,554 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,688 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,688 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.52|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:b56a29e6-5fa5-47c2-abed-de2e51d87228,timestamp:1628278472\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,689 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,689 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,689 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,689 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,795 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,795 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.4|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:985979b0-75a3-4860-bf1c-cf66dbe5cad8,timestamp:1628278472\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,795 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,795 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,795 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,796 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,880 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,880 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,880 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,880 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.71|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:57130c4d-a33a-4751-9402-cc14e2e81350,timestamp:1628278472\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,880 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,880 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,971 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.63|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:ebbe7b0a-1e13-4954-8c1a-5141dfeedf01,timestamp:1628278472\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,971 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,971 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,971 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,972 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:32,972 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,050 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,050 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.32|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:9d514ac1-bdd3-4977-a88c-3b75f339397e,timestamp:1628278473\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,050 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,050 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,050 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,050 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,295 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,295 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,295 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,390 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,390 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.83|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:25292394-a306-448e-8e2d-303ebe3525ec,timestamp:1628278472\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,390 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,390 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,390 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,390 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,467 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,467 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.65|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:5d08df58-cb09-4a7a-bdd0-2da648520a48,timestamp:1628278472\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,467 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,467 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,467 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,467 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,553 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,553 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.31|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:2fca53ad-e0b2-4e76-b9d8-08d161c4254d,timestamp:1628278472\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,553 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,553 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,554 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,554 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,688 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,688 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.52|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:b56a29e6-5fa5-47c2-abed-de2e51d87228,timestamp:1628278472\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,689 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,689 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,689 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,689 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,795 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,795 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.4|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:985979b0-75a3-4860-bf1c-cf66dbe5cad8,timestamp:1628278472\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,795 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,795 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,795 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,796 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,880 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,880 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,880 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,880 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.71|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:57130c4d-a33a-4751-9402-cc14e2e81350,timestamp:1628278472\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,880 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,880 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,971 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.63|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:ebbe7b0a-1e13-4954-8c1a-5141dfeedf01,timestamp:1628278472\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,971 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,971 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,971 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,972 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:32,972 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,050 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,050 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.32|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:9d514ac1-bdd3-4977-a88c-3b75f339397e,timestamp:1628278473\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,050 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,050 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,050 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,050 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,183 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,183 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,183 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,183 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.47|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:6083357b-8c88-4ab8-9560-993789d49d2e,timestamp:1628278473\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,183 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,183 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,278 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,278 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.32|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:44f52540-3c68-45c9-a72d-84a57b96c70e,timestamp:1628278473\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,278 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,278 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,278 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,278 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,376 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,377 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.33|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:8a805970-2987-472e-95e5-44e0c8dc5a71,timestamp:1628278473\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,377 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,377 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,377 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,377 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,461 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,461 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:3.3|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:9920687c-5a86-432d-af39-38774383c888,timestamp:1628278473\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,461 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,461 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,462 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,462 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,557 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,557 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.44|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:400192dd-3b6d-4394-bb56-7799f3e4c5d3,timestamp:1628278473\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,557 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,557 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,557 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,557 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,663 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,663 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.35|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:6805307f-6ff8-47c1-92c6-8a8f65566540,timestamp:1628278473\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,664 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,664 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,664 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,664 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,753 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,183 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,183 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,183 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,183 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.47|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:6083357b-8c88-4ab8-9560-993789d49d2e,timestamp:1628278473\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,183 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,183 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,278 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,278 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.32|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:44f52540-3c68-45c9-a72d-84a57b96c70e,timestamp:1628278473\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,278 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,278 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,278 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,278 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,376 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,377 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.33|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:8a805970-2987-472e-95e5-44e0c8dc5a71,timestamp:1628278473\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,377 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,377 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,377 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,377 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,461 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,461 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:3.3|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:9920687c-5a86-432d-af39-38774383c888,timestamp:1628278473\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,461 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,461 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,462 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,462 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,557 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,557 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.44|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:400192dd-3b6d-4394-bb56-7799f3e4c5d3,timestamp:1628278473\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,557 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,557 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,557 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,557 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,663 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,663 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.35|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:6805307f-6ff8-47c1-92c6-8a8f65566540,timestamp:1628278473\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,664 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,664 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,664 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,664 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,753 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,753 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.32|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:ae08e903-5be2-4133-a26e-5fc97763d8ba,timestamp:1628278473\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,753 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,753 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,754 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,754 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,837 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.42|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:724cbace-54ce-47fe-ac02-8fa819f632f9,timestamp:1628278473\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,838 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,838 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,838 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,838 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,838 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,953 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,954 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:4.59|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:6cba3a5b-90ff-40db-9980-5cee6fb8cef1,timestamp:1628278473\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,954 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 6\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,954 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,954 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:33,955 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,042 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,042 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.34|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:fb27477b-32ae-4d3b-a534-b71ab58931d3,timestamp:1628278474\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,042 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,042 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,042 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,042 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,753 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.32|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:ae08e903-5be2-4133-a26e-5fc97763d8ba,timestamp:1628278473\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,753 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,753 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,754 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,754 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,837 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.42|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:724cbace-54ce-47fe-ac02-8fa819f632f9,timestamp:1628278473\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,838 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,838 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,838 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,838 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,838 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,953 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,954 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:4.59|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:6cba3a5b-90ff-40db-9980-5cee6fb8cef1,timestamp:1628278473\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,954 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 6\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,954 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,954 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:33,955 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,042 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,042 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.34|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:fb27477b-32ae-4d3b-a534-b71ab58931d3,timestamp:1628278474\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,042 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,042 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,042 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,042 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,146 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,146 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,146 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,146 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.32|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:df34e6d7-e0ee-45ae-9591-a21308ff77b7,timestamp:1628278474\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,146 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,146 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,250 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,250 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.43|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:54c021f4-22f9-441b-81f8-adc876f1e7cc,timestamp:1628278474\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,250 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,250 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,250 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,250 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,339 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.81|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:71c249f5-919c-4a47-b960-d8b92922f550,timestamp:1628278474\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,340 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,340 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,340 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,340 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,340 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,146 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,146 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,146 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,146 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.32|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:df34e6d7-e0ee-45ae-9591-a21308ff77b7,timestamp:1628278474\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,146 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,146 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,250 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,250 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.43|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:54c021f4-22f9-441b-81f8-adc876f1e7cc,timestamp:1628278474\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,250 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,250 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,250 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,250 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,339 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.81|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:71c249f5-919c-4a47-b960-d8b92922f550,timestamp:1628278474\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,340 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,340 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,340 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,340 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,340 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,482 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,482 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,482 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.33|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:265f3bb9-c5d9-457b-9e14-6ffe1a352c86,timestamp:1628278474\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,482 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,483 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,483 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,572 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,573 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,573 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.89|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:1f3e8ea5-52ce-4ac5-b25f-52b6a81c84fe,timestamp:1628278474\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,573 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,573 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:34:34,573 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,482 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,482 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,482 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.33|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:265f3bb9-c5d9-457b-9e14-6ffe1a352c86,timestamp:1628278474\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,482 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,483 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,483 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,572 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,573 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:43712 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,573 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.89|#ModelName:model,Level:Model|#hostname:f74e90b8f6d5,requestID:1f3e8ea5-52ce-4ac5-b25f-52b6a81c84fe,timestamp:1628278474\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,573 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,573 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06 19:34:34,573 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:f74e90b8f6d5,timestamp:null\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transformer.transform(\n",
    "    data=inference_inputs,\n",
    "    data_type=\"S3Prefix\",\n",
    "    content_type=\"application/x-image\",\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2f97c1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### input images by manifest file\n",
    "First, we generate a manifest file. Then we use the manifest file containing a list of object keys that you want to batch inference. Some key points:\n",
    "- content_type = 'application/x-image' (!!! here the content_type is for the actual object to be inference, not for the manifest file)\n",
    "- data_type = 'ManifestFile'\n",
    "- Manifest file format must follow the format as [this document](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_S3DataSource.html#SageMaker-Type-S3DataSource-S3DataType) pointed out. We create the manifest file by using jsonlines package.\n",
    "``` json\n",
    "[ {\"prefix\": \"s3://customer_bucket/some/prefix/\"},\n",
    "\"relative/path/to/custdata-1\",\n",
    "\"relative/path/custdata-2\",\n",
    "...\n",
    "\"relative/path/custdata-N\"\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "273bed03",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33e30699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_files\n",
      " ['99.png', '51.png', '97.png', '2.png', '5.png', '46.png', '35.png', '41.png', '4.png', '90.png', '58.png', '30.png', '75.png', '68.png', '17.png', '92.png', '54.png', '21.png', '32.png', '38.png', '52.png', '67.png', '94.png', '31.png', '66.png', '37.png', '77.png', '9.png', '60.png', '39.png', '44.png', '73.png', '22.png', '74.png', '96.png', '65.png', '3.png', '25.png', '64.png', '91.png', '16.png', '45.png', '8.png', '72.png', '71.png', '42.png', '63.png', '87.png', '70.png', '61.png', '1.png', '83.png', '49.png', '50.png', '28.png', '79.png', '7.png', '47.png', '62.png', '93.png', '0.png', '56.png', '55.png', '82.png', '89.png', '12.png', '14.png', '98.png', '78.png', '23.png', '85.png', '19.png', '86.png', '80.png', '76.png', '43.png', '10.png', '36.png', '27.png', '33.png', '48.png', '81.png', '20.png', '95.png', '84.png', '69.png', '11.png', '29.png', '59.png', '34.png', '57.png', '15.png', '13.png', '24.png', '18.png', '40.png', '88.png', '26.png', '53.png', '6.png']\n",
      "manifest_content\n",
      " [{'prefix': 's3://sagemaker-us-east-1-804604702169/sagemaker/DEMO-pytorch-batch-inference-script/images/'}, '99.png', '51.png', '97.png', '2.png', '5.png', '46.png', '35.png', '41.png', '4.png', '90.png', '58.png', '30.png', '75.png', '68.png', '17.png', '92.png', '54.png', '21.png', '32.png', '38.png', '52.png', '67.png', '94.png', '31.png', '66.png', '37.png', '77.png', '9.png', '60.png', '39.png', '44.png', '73.png', '22.png', '74.png', '96.png', '65.png', '3.png', '25.png', '64.png', '91.png', '16.png', '45.png', '8.png', '72.png', '71.png', '42.png', '63.png', '87.png', '70.png', '61.png', '1.png', '83.png', '49.png', '50.png', '28.png', '79.png', '7.png', '47.png', '62.png', '93.png', '0.png', '56.png', '55.png', '82.png', '89.png', '12.png', '14.png', '98.png', '78.png', '23.png', '85.png', '19.png', '86.png', '80.png', '76.png', '43.png', '10.png', '36.png', '27.png', '33.png', '48.png', '81.png', '20.png', '95.png', '84.png', '69.png', '11.png', '29.png', '59.png', '34.png', '57.png', '15.png', '13.png', '24.png', '18.png', '40.png', '88.png', '26.png', '53.png', '6.png']\n",
      "manifest_obj\n",
      " s3://sagemaker-us-east-1-804604702169/sagemaker/DEMO-pytorch-batch-inference-script/manifest.json\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "\n",
    "# build image list\n",
    "manifest_prefix = f\"s3://{bucket}/{prefix}/images/\"\n",
    "\n",
    "path = image_dir\n",
    "img_files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "print(\"img_files\\n\", img_files)\n",
    "\n",
    "manifest_content = [{\"prefix\": manifest_prefix}]\n",
    "manifest_content.extend(img_files)\n",
    "\n",
    "print(\"manifest_content\\n\", manifest_content)\n",
    "\n",
    "# write jsonl file\n",
    "manifest_file = \"manifest.json\"\n",
    "with jsonlines.open(manifest_file, mode=\"w\") as writer:\n",
    "    writer.write(manifest_content)\n",
    "\n",
    "# upload to S3\n",
    "manifest_obj = sagemaker_session.upload_data(path=manifest_file, key_prefix=prefix)\n",
    "\n",
    "print(\"manifest_obj\\n\", manifest_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5bd71dd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# batch transform with manifest file\n",
    "transform_job = transformer.transform(\n",
    "    data=manifest_obj,\n",
    "    data_type=\"ManifestFile\",\n",
    "    content_type=\"application/x-image\",\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ed1e73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest transform job \n",
      " pytorch-inference-2021-08-06-19-36-23-811\n"
     ]
    }
   ],
   "source": [
    "print(\"latest transform job \\n\", transformer.latest_transform_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9e7e1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CreationTime': datetime.datetime(2021, 8, 6, 19, 36, 24, 50000, tzinfo=tzlocal()),\n",
      " 'DataProcessing': {'InputFilter': '$',\n",
      "                    'JoinSource': 'None',\n",
      "                    'OutputFilter': '$'},\n",
      " 'ModelName': 'pytorch-inference-2021-08-06-19-29-06-195',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '869',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Fri, 06 Aug 2021 19:36:36 GMT',\n",
      "                                      'x-amzn-requestid': '38fb23b3-84e5-49b8-a35b-7541906f7a73'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '38fb23b3-84e5-49b8-a35b-7541906f7a73',\n",
      "                      'RetryAttempts': 0},\n",
      " 'TransformInput': {'CompressionType': 'None',\n",
      "                    'ContentType': 'application/x-image',\n",
      "                    'DataSource': {'S3DataSource': {'S3DataType': 'ManifestFile',\n",
      "                                                    'S3Uri': 's3://sagemaker-us-east-1-804604702169/sagemaker/DEMO-pytorch-batch-inference-script/manifest.json'}},\n",
      "                    'SplitType': 'None'},\n",
      " 'TransformJobArn': 'arn:aws:sagemaker:us-east-1:804604702169:transform-job/pytorch-inference-2021-08-06-19-36-23-811',\n",
      " 'TransformJobName': 'pytorch-inference-2021-08-06-19-36-23-811',\n",
      " 'TransformJobStatus': 'InProgress',\n",
      " 'TransformOutput': {'AssembleWith': 'None',\n",
      "                     'KmsKeyId': '',\n",
      "                     'S3OutputPath': 's3://sagemaker-us-east-1-804604702169/pytorch-inference-2021-08-06-19-36-23-811'},\n",
      " 'TransformResources': {'InstanceCount': 1, 'InstanceType': 'ml.c5.xlarge'}}\n"
     ]
    }
   ],
   "source": [
    "# look at the status of the transform job\n",
    "import boto3\n",
    "import pprint as pp\n",
    "\n",
    "sm_cli = boto3.client(\"sagemaker\")\n",
    "\n",
    "res = sm_cli.describe_transform_job(TransformJobName=transformer.latest_transform_job.name)\n",
    "\n",
    "pp.pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0400823a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "###  Multiple instance\n",
    "We use `instance_count > 1` to create multiple inference instances. When a batch transform job starts, Amazon SageMaker initializes compute instances and distributes the inference or preprocessing workload between them. Batch Transform partitions the Amazon S3 objects in the input by key and maps Amazon S3 objects to instances. When you have multiples files, one instance might process input1.csv, and another instance might process the file named input2.csv.\n",
    "\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da0c4fad",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............................\u001b[34m2021-08-06 19:41:44,973 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[34mTorchserve version: 0.3.0\u001b[0m\n",
      "\u001b[34mTS Home: /opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[34mMax heap size: 912 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.6\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mInitial Models: model.mar\u001b[0m\n",
      "\u001b[34mLog dir: /logs\u001b[0m\n",
      "\u001b[34mMetrics dir: /logs\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 4\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[34mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[34mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[34mEnable metrics API: true\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,013 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,037 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag a83db830e3ba4d42aeb6dfdfd37f07bb\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,049 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,066 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,280 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,276 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,283 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]45\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,283 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,283 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,284 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]46\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,284 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,286 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,296 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,301 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,321 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,327 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]48\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,327 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,328 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,328 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,354 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,355 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]47\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,356 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,356 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,356 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,365 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,365 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,366 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,373 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,373 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,373 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:45,374 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:46,213 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:562763372dec,timestamp:1628278906\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:46,215 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:48.24006271362305|#Level:Host|#hostname:562763372dec,timestamp:1628278906\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:46,215 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:7.675849914550781|#Level:Host|#hostname:562763372dec,timestamp:1628278906\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:46,216 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:13.7|#Level:Host|#hostname:562763372dec,timestamp:1628278906\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:46,216 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:6276.1171875|#Level:Host|#hostname:562763372dec,timestamp:1628278906\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:46,217 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:923.91796875|#Level:Host|#hostname:562763372dec,timestamp:1628278906\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:46,217 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:16.0|#Level:Host|#hostname:562763372dec,timestamp:1628278906\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:46,738 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1247\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:46,739 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:1682|#Level:Host|#hostname:562763372dec,timestamp:1628278906\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:46,739 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:104|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:46,787 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1296\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:46,788 [INFO ] W-9002-model_1 TS_METRICS - W-9002-model_1.ms:1727|#Level:Host|#hostname:562763372dec,timestamp:1628278906\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:46,788 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:104|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:46,815 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1282\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:46,815 [INFO ] W-9001-model_1 TS_METRICS - W-9001-model_1.ms:1754|#Level:Host|#hostname:562763372dec,timestamp:1628278906\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:46,815 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:145|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:46,877 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1350\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:46,878 [INFO ] W-9003-model_1 TS_METRICS - W-9003-model_1.ms:1817|#Level:Host|#hostname:562763372dec,timestamp:1628278906\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:46,879 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:141|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,148 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[34mTorchserve version: 0.3.0\u001b[0m\n",
      "\u001b[34mTS Home: /opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[34mMax heap size: 908 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.6\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mInitial Models: model.mar\u001b[0m\n",
      "\u001b[34mLog dir: /logs\u001b[0m\n",
      "\u001b[34mMetrics dir: /logs\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 4\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[34mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[34mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[34mEnable metrics API: true\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,184 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,204 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 370820c9ceb1428db40cd3864085ef26\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,215 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,243 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,418 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,421 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]43\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,421 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,421 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,426 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,427 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]42\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,428 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,428 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,430 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,431 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,479 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,480 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]47\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,480 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,480 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,481 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,484 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,484 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]48\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,485 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,485 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,485 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,521 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,522 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,524 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,538 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,539 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,541 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:55,542 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:56,263 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:3c32b541160f,timestamp:1628278916\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:56,265 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:48.2401237487793|#Level:Host|#hostname:3c32b541160f,timestamp:1628278916\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:56,265 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:7.675788879394531|#Level:Host|#hostname:3c32b541160f,timestamp:1628278916\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:56,265 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:13.7|#Level:Host|#hostname:3c32b541160f,timestamp:1628278916\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:56,265 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:6301.84375|#Level:Host|#hostname:3c32b541160f,timestamp:1628278916\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:56,266 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:895.3984375|#Level:Host|#hostname:3c32b541160f,timestamp:1628278916\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:56,266 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:15.6|#Level:Host|#hostname:3c32b541160f,timestamp:1628278916\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:56,793 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1130\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:56,794 [INFO ] W-9001-model_1 TS_METRICS - W-9001-model_1.ms:1569|#Level:Host|#hostname:3c32b541160f,timestamp:1628278916\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:56,794 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:102|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:56,810 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1152\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:56,811 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:1589|#Level:Host|#hostname:3c32b541160f,timestamp:1628278916\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:56,811 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:96|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:56,833 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1170\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:56,833 [INFO ] W-9002-model_1 TS_METRICS - W-9002-model_1.ms:1608|#Level:Host|#hostname:3c32b541160f,timestamp:1628278916\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:56,833 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:112|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:56,901 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1244\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:56,901 [INFO ] W-9003-model_1 TS_METRICS - W-9003-model_1.ms:1675|#Level:Host|#hostname:3c32b541160f,timestamp:1628278916\u001b[0m\n",
      "\u001b[34m2021-08-06 19:41:56,902 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:96|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m2021-08-06 19:42:03,257 [INFO ] pool-1-thread-5 ACCESS_LOG - /169.254.255.130:46874 \"GET /ping HTTP/1.1\" 200 20\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,257 [INFO ] pool-1-thread-5 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,278 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:46880 \"GET /execution-parameters HTTP/1.1\" 404 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,278 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,408 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 15\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,408 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 24\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,408 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:13.35|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:7f87c817-6d4e-489a-95dd-d7d086d42908,timestamp:1628278923\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,408 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,409 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,410 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,517 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 16\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,518 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 20\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,517 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:14.79|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:7fb67b16-a3c0-4dd5-923b-09063e1a35bb,timestamp:1628278923\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,518 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,257 [INFO ] pool-1-thread-5 ACCESS_LOG - /169.254.255.130:46874 \"GET /ping HTTP/1.1\" 200 20\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,257 [INFO ] pool-1-thread-5 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,278 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:46880 \"GET /execution-parameters HTTP/1.1\" 404 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,278 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,408 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 15\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,408 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 24\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,408 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:13.35|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:7f87c817-6d4e-489a-95dd-d7d086d42908,timestamp:1628278923\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,408 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,409 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,410 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,517 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 16\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,518 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 20\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,517 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:14.79|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:7fb67b16-a3c0-4dd5-923b-09063e1a35bb,timestamp:1628278923\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,518 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,519 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,519 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,706 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,707 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:12.46|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:4f7b4d68-42d1-4c8e-9154-179febdfeb2c,timestamp:1628278923\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,707 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 15\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,707 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,707 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,707 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,801 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 12\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,801 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:10.58|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:4442c678-5c64-4a73-a288-b63051e23ee1,timestamp:1628278923\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,802 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 14\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,803 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,803 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,803 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,867 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.64|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:429f36b3-8720-4fec-9d9b-76b9b2ce4e48,timestamp:1628278923\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,868 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,869 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 5\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,869 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,869 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:03,869 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,519 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,519 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,706 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,707 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:12.46|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:4f7b4d68-42d1-4c8e-9154-179febdfeb2c,timestamp:1628278923\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,707 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 15\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,707 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,707 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,707 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,801 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 12\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,801 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:10.58|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:4442c678-5c64-4a73-a288-b63051e23ee1,timestamp:1628278923\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,802 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 14\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,803 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,803 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,803 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,867 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.64|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:429f36b3-8720-4fec-9d9b-76b9b2ce4e48,timestamp:1628278923\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,868 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,869 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 5\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,869 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,869 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:03,869 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:03,934 [INFO ] pool-1-thread-5 ACCESS_LOG - /169.254.255.130:35918 \"GET /ping HTTP/1.1\" 200 15\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:03,935 [INFO ] pool-1-thread-5 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:03,961 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:35926 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:03,961 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,051 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.57|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:9739e80e-a2d4-49a4-8fce-76f2490c080c,timestamp:1628278924\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,051 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.57|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:9739e80e-a2d4-49a4-8fce-76f2490c080c,timestamp:1628278924\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,051 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,051 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,051 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,052 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,052 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,204 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,204 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:2.03|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:2d969397-0206-4fe4-a596-3479b0b2d571,timestamp:1628278924\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,205 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,205 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,205 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,205 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,297 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,297 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,297 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,298 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,298 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,297 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.61|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:e9edea57-5de6-4140-a596-ba7abd774573,timestamp:1628278924\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,382 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:2.15|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:898381cc-3df8-4c64-a4e4-13ad387a6612,timestamp:1628278924\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,382 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,383 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 5\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,383 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,383 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,384 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,487 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,487 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.82|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:62832af4-9d4c-4300-8189-e1ba58517922,timestamp:1628278924\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,488 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,488 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,488 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,488 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,570 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,571 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,570 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.63|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:f6de0866-9feb-4af3-a7e9-a0b7eab3472b,timestamp:1628278924\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,571 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,571 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,572 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,657 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,657 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.63|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:18658669-d0a4-4c42-81b1-4349925dba48,timestamp:1628278924\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,657 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,658 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,658 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,051 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,051 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,051 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,052 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,052 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,204 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,204 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:2.03|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:2d969397-0206-4fe4-a596-3479b0b2d571,timestamp:1628278924\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,205 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,205 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,205 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,205 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,297 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,297 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,297 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,298 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,298 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,297 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.61|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:e9edea57-5de6-4140-a596-ba7abd774573,timestamp:1628278924\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,382 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:2.15|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:898381cc-3df8-4c64-a4e4-13ad387a6612,timestamp:1628278924\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,382 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,383 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 5\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,383 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,383 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,384 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,487 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,487 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.82|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:62832af4-9d4c-4300-8189-e1ba58517922,timestamp:1628278924\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,488 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,488 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,488 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,488 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,570 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,571 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,570 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.63|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:f6de0866-9feb-4af3-a7e9-a0b7eab3472b,timestamp:1628278924\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,571 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,571 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,572 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,657 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,657 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.63|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:18658669-d0a4-4c42-81b1-4349925dba48,timestamp:1628278924\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,657 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,658 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,658 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,658 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,732 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.52|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:dae92b50-b927-4dfd-af6a-f0ac4502aa8e,timestamp:1628278924\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,732 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,733 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,733 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,733 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,733 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,814 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,814 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:2.0|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:6a9bc638-7940-4a1a-ad73-6064fb6d307d,timestamp:1628278924\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,814 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,814 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,815 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,815 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,968 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.97|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:b5245bf3-6bb5-4ba4-bceb-b55b1fbbb566,timestamp:1628278924\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,968 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,969 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 6\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,969 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,969 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:04,969 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,658 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,732 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.52|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:dae92b50-b927-4dfd-af6a-f0ac4502aa8e,timestamp:1628278924\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,732 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,733 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,733 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,733 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,733 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,814 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,814 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:2.0|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:6a9bc638-7940-4a1a-ad73-6064fb6d307d,timestamp:1628278924\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,814 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,814 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,815 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,815 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,968 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.97|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:b5245bf3-6bb5-4ba4-bceb-b55b1fbbb566,timestamp:1628278924\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,968 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,969 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 6\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,969 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,969 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:04,969 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:04,432 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 15\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:04,432 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 23\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:04,433 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:04,433 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:04,433 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:04,435 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:13.72|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:9037b66e-7f0c-4c60-b4bc-6a379256fdf8,timestamp:1628278924\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:04,506 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 14\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:04,506 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 16\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:04,506 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:13.71|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:d5f86abd-e9b1-43ad-b829-040d64747409,timestamp:1628278924\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:04,507 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:04,507 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:04,507 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:04,896 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 14\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:04,897 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 17\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:04,897 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:14.09|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:6e423ca3-45c6-44d3-8fd5-e4183f93aa32,timestamp:1628278924\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:04,897 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:04,897 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:04,897 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:04,999 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,000 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 15\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,000 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,000 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:11.66|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:1f9eabed-da7a-40de-9497-c1ac32f756b5,timestamp:1628278924\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,000 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,000 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,049 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,049 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.95|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:0312205f-e4ba-4843-891f-bcbeda19abef,timestamp:1628278925\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,049 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,049 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,049 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,049 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,142 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,142 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.64|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:d9723024-cb42-4a05-a26f-ddd996406d24,timestamp:1628278925\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,142 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,142 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,049 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,049 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.95|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:0312205f-e4ba-4843-891f-bcbeda19abef,timestamp:1628278925\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,049 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,049 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,049 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,049 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,142 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,142 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.64|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:d9723024-cb42-4a05-a26f-ddd996406d24,timestamp:1628278925\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,142 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,142 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,143 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,143 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,231 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,231 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,231 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,231 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,232 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,233 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.92|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:bafb0f94-60cf-4234-815a-9713f542fb9e,timestamp:1628278925\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,316 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,316 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.42|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:02c8aea9-af2b-48e2-83fd-c6f60b8c37b6,timestamp:1628278925\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,316 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,316 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,316 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,317 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,430 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.81|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:f1262e46-aa97-462b-a7da-1a35fb235fd2,timestamp:1628278925\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,430 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,430 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,430 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,431 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,431 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,527 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,527 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.8|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:d0de590f-b702-40bc-9c39-f44ba7dc9bbd,timestamp:1628278925\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,527 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,527 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,528 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,528 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,618 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,618 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.44|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:1e84e8d6-1573-42bd-ba41-88b76b7f3f08,timestamp:1628278925\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,618 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,143 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,143 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,231 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,231 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,231 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,231 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,232 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,233 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.92|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:bafb0f94-60cf-4234-815a-9713f542fb9e,timestamp:1628278925\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,316 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,316 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.42|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:02c8aea9-af2b-48e2-83fd-c6f60b8c37b6,timestamp:1628278925\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,316 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,316 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,316 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,317 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,430 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.81|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:f1262e46-aa97-462b-a7da-1a35fb235fd2,timestamp:1628278925\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,430 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,430 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,430 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,431 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,431 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,527 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,527 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.8|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:d0de590f-b702-40bc-9c39-f44ba7dc9bbd,timestamp:1628278925\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,527 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,527 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,528 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,528 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,618 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,618 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.44|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:1e84e8d6-1573-42bd-ba41-88b76b7f3f08,timestamp:1628278925\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,618 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,619 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,619 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,619 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,698 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.78|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:3f3c938b-9a1e-45a2-b0ce-369e62d32d1c,timestamp:1628278925\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,699 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,699 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,699 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,700 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,700 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,619 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,619 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,619 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,698 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.78|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:3f3c938b-9a1e-45a2-b0ce-369e62d32d1c,timestamp:1628278925\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,699 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,699 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,699 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,700 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,700 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,789 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,789 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.93|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:8159792e-97a6-4f2b-ac0f-91f71a7dab09,timestamp:1628278925\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,790 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,790 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-08-06T19:42:03.285:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,789 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,789 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.93|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:8159792e-97a6-4f2b-ac0f-91f71a7dab09,timestamp:1628278925\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,790 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,790 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[36m2021-08-06T19:42:03.285:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,790 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,790 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,902 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,902 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,902 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,902 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,902 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,903 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.91|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:35f4a53f-746f-4b73-833c-d3f83a207388,timestamp:1628278925\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,973 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,973 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.92|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:17834ba9-58cb-4d66-a6db-3d0bb1218bea,timestamp:1628278925\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,973 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,973 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,974 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:05,974 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,790 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,790 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,902 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,902 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,902 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,902 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,902 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,903 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.91|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:35f4a53f-746f-4b73-833c-d3f83a207388,timestamp:1628278925\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,973 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,973 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.92|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:17834ba9-58cb-4d66-a6db-3d0bb1218bea,timestamp:1628278925\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,973 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,973 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,974 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:05,974 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,153 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,154 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.67|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:5aceb6d8-238d-4921-8a37-b5d87e060fbb,timestamp:1628278925\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,154 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,154 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,154 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,154 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,261 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,261 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.6|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:518b5d1a-f6af-4df6-b9b3-60ec635e5f40,timestamp:1628278925\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,261 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,261 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,261 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,262 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,354 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.56|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:862f6a02-221c-4511-baa0-d6bd99bd41d6,timestamp:1628278925\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,354 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,354 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,354 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,354 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,355 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,448 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,449 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,448 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.8|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:5df8fd23-7b1d-4cb9-9c43-67f976b34717,timestamp:1628278925\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,449 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,450 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,450 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,535 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,535 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:2.12|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:c0d45bb5-cb67-4a78-b6ac-897e07fcd248,timestamp:1628278925\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,535 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,535 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,536 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,536 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,668 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:2.19|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:f0b74c9a-5c28-4483-a5e5-06cb11ab60c0,timestamp:1628278925\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,669 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,669 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 5\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,669 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,670 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,670 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,838 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,838 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.73|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:2a96e0cd-9143-4aae-8d19-d12798449cec,timestamp:1628278925\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,838 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,839 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,839 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,839 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,963 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,963 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.72|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:85e99f03-3727-4878-bcd6-9b66013d1f26,timestamp:1628278925\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,963 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,963 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,963 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:05,964 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,054 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,054 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.7|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:c4c751ba-fe8d-4cdc-919b-f93f7409750a,timestamp:1628278926\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,054 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,054 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,054 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,054 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06T19:42:03.968:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,052 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.71|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:b73fbc54-e3cd-4046-803f-63022a2f1307,timestamp:1628278926\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,052 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,052 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,052 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,052 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.71|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:b73fbc54-e3cd-4046-803f-63022a2f1307,timestamp:1628278926\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,052 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,052 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,052 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,053 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,053 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,142 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,143 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.7|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:f2e4caee-b2bc-4d95-8328-17ca68b99db8,timestamp:1628278926\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,143 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,143 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,144 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,144 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,234 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.41|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:f90ecc4f-e3b5-4480-aa7f-c399554376cd,timestamp:1628278926\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,234 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,234 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,234 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,235 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,235 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,328 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,328 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.51|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:bf2d85a6-0cd8-4fd2-b4da-4723bd24b797,timestamp:1628278926\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,328 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,328 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,329 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,329 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,415 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,415 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.62|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:eb6cda65-e642-4d31-b751-ec9a85f9854e,timestamp:1628278926\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,415 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,415 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,415 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,415 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,497 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,497 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:2.01|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:12d03e75-0974-4e3f-9696-d23206acf077,timestamp:1628278926\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,498 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,498 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,498 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,499 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,642 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:2.04|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:71954ba6-ce65-49ef-b393-5e396c720532,timestamp:1628278926\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,643 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,643 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,643 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,644 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,644 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,053 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,053 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,142 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,143 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.7|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:f2e4caee-b2bc-4d95-8328-17ca68b99db8,timestamp:1628278926\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,143 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,143 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,144 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,144 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,234 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.41|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:f90ecc4f-e3b5-4480-aa7f-c399554376cd,timestamp:1628278926\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,234 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,234 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,234 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,235 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,235 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,328 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,328 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.51|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:bf2d85a6-0cd8-4fd2-b4da-4723bd24b797,timestamp:1628278926\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,328 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,328 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,329 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,329 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,415 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,415 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.62|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:eb6cda65-e642-4d31-b751-ec9a85f9854e,timestamp:1628278926\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,415 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,415 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,415 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,415 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,497 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,497 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:2.01|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:12d03e75-0974-4e3f-9696-d23206acf077,timestamp:1628278926\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,498 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,498 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,498 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,499 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,642 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:2.04|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:71954ba6-ce65-49ef-b393-5e396c720532,timestamp:1628278926\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,643 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,643 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,643 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,644 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,644 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,728 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,728 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.43|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:ea288b2c-7c6e-4768-b874-77df57a2a0b9,timestamp:1628278926\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,729 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,729 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,729 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,729 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,821 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,821 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.57|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:4528b00c-fcc5-4846-a42e-597acdf80167,timestamp:1628278926\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,821 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,822 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,822 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,822 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,895 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,895 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,896 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,895 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.61|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:0e6e0bf7-122a-4758-b8b3-bd45ab20c0d8,timestamp:1628278926\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,896 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,896 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,988 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,988 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.36|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:3b7de672-ed6a-487a-bb34-1382b13e9c92,timestamp:1628278926\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,988 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,988 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,989 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:06,989 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,728 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,728 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.43|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:ea288b2c-7c6e-4768-b874-77df57a2a0b9,timestamp:1628278926\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,729 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,729 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,729 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,729 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,821 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,821 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.57|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:4528b00c-fcc5-4846-a42e-597acdf80167,timestamp:1628278926\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,821 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,822 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,822 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,822 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,895 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,895 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,896 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,895 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.61|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:0e6e0bf7-122a-4758-b8b3-bd45ab20c0d8,timestamp:1628278926\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,896 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,896 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,988 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,988 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.36|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:3b7de672-ed6a-487a-bb34-1382b13e9c92,timestamp:1628278926\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,988 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,988 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,989 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:06,989 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,355 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,355 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.86|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:0d2235ce-a1e8-4b49-8df5-6ebdd650032d,timestamp:1628278926\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,356 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 5\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,356 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,356 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,356 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,433 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,434 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.99|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:2b7ba363-1720-4bf3-8e33-224a71e4b645,timestamp:1628278926\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,434 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,434 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,434 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,434 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,522 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,522 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.71|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:e2885932-dd73-4fc4-abb6-160087960432,timestamp:1628278926\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,522 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,522 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,523 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,523 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,603 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,603 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.8|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:9d0febf7-b265-4d56-8df4-9ce4b0a3125b,timestamp:1628278926\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,603 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,603 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,604 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,604 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,692 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.99|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:ff38bf0a-85bb-4cf4-a75c-38208e0441fa,timestamp:1628278926\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,693 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,693 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 5\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,693 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,693 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,693 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,783 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.66|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:55e31c26-b770-4bad-bb45-edb142aaae7d,timestamp:1628278926\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,783 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,784 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,784 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,784 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,784 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,919 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,920 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,920 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,920 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,920 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:06,921 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.6|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:c7e7c681-131e-4d77-a1cf-f79b70f94ba7,timestamp:1628278926\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,009 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,009 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:2.19|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:033a1937-0768-4715-9b2b-e0e860cb0868,timestamp:1628278927\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,009 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,010 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,010 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,010 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,139 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,140 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.63|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:1202d7e0-8979-4c0c-ae58-6f2aa421ed34,timestamp:1628278927\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,140 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,140 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,140 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,140 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,223 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,223 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.59|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:e12210a8-314b-47e7-9d6f-94c66ab85675,timestamp:1628278927\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,223 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,223 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,224 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,224 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,319 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,319 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.98|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:c2e9f528-6dbb-4e7b-8368-4a7800e923b3,timestamp:1628278927\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,320 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,320 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,320 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,320 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,430 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,431 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.6|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:9e2c72c5-fceb-4334-8afc-24dbf894cf0a,timestamp:1628278927\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,431 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,431 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,431 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,431 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,532 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.48|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:3ff571e5-dfc1-49a5-8f0e-fb3d4dceea73,timestamp:1628278927\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,532 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,532 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,532 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,532 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,533 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,609 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,609 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.64|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:d3f5aa6f-2bfa-4b53-9ba7-fce81b4199b5,timestamp:1628278927\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,609 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,609 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,609 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,609 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,139 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,140 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.63|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:1202d7e0-8979-4c0c-ae58-6f2aa421ed34,timestamp:1628278927\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,140 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,140 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,140 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,140 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,223 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,223 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.59|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:e12210a8-314b-47e7-9d6f-94c66ab85675,timestamp:1628278927\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,223 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,223 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,224 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,224 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,319 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,319 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.98|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:c2e9f528-6dbb-4e7b-8368-4a7800e923b3,timestamp:1628278927\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,320 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,320 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,320 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,320 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,430 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,431 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.6|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:9e2c72c5-fceb-4334-8afc-24dbf894cf0a,timestamp:1628278927\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,431 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,431 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,431 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,431 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,532 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.48|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:3ff571e5-dfc1-49a5-8f0e-fb3d4dceea73,timestamp:1628278927\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,532 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,532 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,532 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,532 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,533 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,609 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,609 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.64|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:d3f5aa6f-2bfa-4b53-9ba7-fce81b4199b5,timestamp:1628278927\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,609 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,609 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,609 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,609 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,686 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,687 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,687 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.42|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:c7a09f60-6c07-4442-8f1d-89e3c9eaf796,timestamp:1628278927\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,687 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,687 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,688 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,761 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,761 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.54|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:cf83009e-690a-4e62-8d98-2efc995aaadf,timestamp:1628278927\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,761 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,761 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,762 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,762 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,893 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.5|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:1710b14e-6a5f-431b-9e01-d0d39cffeb4b,timestamp:1628278927\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,893 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,894 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,894 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,894 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,894 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,981 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,981 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.55|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:667f7620-e4e8-45c2-98d5-5547535d96b8,timestamp:1628278927\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,981 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,981 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,982 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:07,982 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,686 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,687 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,687 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.42|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:c7a09f60-6c07-4442-8f1d-89e3c9eaf796,timestamp:1628278927\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,687 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,687 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,688 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,761 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,761 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.54|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:cf83009e-690a-4e62-8d98-2efc995aaadf,timestamp:1628278927\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,761 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,761 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,762 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,762 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,893 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.5|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:1710b14e-6a5f-431b-9e01-d0d39cffeb4b,timestamp:1628278927\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,893 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,894 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,894 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,894 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,894 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,981 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,981 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.55|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:667f7620-e4e8-45c2-98d5-5547535d96b8,timestamp:1628278927\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,981 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,981 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,982 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:07,982 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,400 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:2.09|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:2715be31-2089-4e4c-8f23-5d7d758898da,timestamp:1628278927\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,401 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,401 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,401 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,401 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,401 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,509 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,509 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,509 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,509 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.95|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:a0241bb2-edb1-4e59-9246-5e2cf68e6d5b,timestamp:1628278927\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,510 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,510 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,593 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,594 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.66|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:851d8ffa-4473-416f-8973-414b31b7d633,timestamp:1628278927\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,594 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,594 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,594 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,594 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,694 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,694 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.68|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:e63d3a1a-4c35-476e-b068-9569a0759807,timestamp:1628278927\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,695 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,695 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,695 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,695 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,813 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,813 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.82|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:7dbb1d96-2da3-41a7-b44a-0d791554697f,timestamp:1628278927\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,813 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,813 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,814 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,814 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,960 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,960 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.49|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:58547a04-f64c-4a01-84cb-084a5ca2081d,timestamp:1628278927\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,961 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,961 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,961 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:07,961 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,130 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,131 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,131 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,131 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.66|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:8cff3bd0-5566-4b25-9136-2ef09eb5bdfc,timestamp:1628278928\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,131 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,131 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:08,061 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:08,061 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.4|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:f99c825a-b0a8-4305-9193-c89fd21e068d,timestamp:1628278928\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:08,061 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:08,061 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:08,061 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:08,062 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:08,164 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:08,164 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:08,164 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.63|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:4cb5126b-d29e-4220-b355-ab7cc018ecae,timestamp:1628278928\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:08,164 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:08,164 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:08,164 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:08,235 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:08,235 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.43|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:2eca1a9d-d48b-4bbd-bd44-97710c409c34,timestamp:1628278928\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:08,235 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:08,235 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:08,236 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-08-06 19:42:08,236 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:08,061 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:08,061 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.4|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:f99c825a-b0a8-4305-9193-c89fd21e068d,timestamp:1628278928\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:08,061 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:08,061 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:08,061 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:08,062 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:08,164 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:08,164 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:08,164 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.63|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:4cb5126b-d29e-4220-b355-ab7cc018ecae,timestamp:1628278928\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:08,164 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:08,164 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2021-08-06 19:42:08,164 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:08,235 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:08,235 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.43|#ModelName:model,Level:Model|#hostname:3c32b541160f,requestID:2eca1a9d-d48b-4bbd-bd44-97710c409c34,timestamp:1628278928\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:08,235 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:46888 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:08,235 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:08,236 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-08-06 19:42:08,236 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:3c32b541160f,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,239 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,239 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.91|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:335b0f4c-4592-4bca-9cf8-df81975fc843,timestamp:1628278928\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,240 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,240 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,240 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,240 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,345 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,346 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,345 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.53|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:ee16e6eb-2ff4-4fcb-a97d-46ad5352fd4a,timestamp:1628278928\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,346 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,346 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,346 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,490 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,491 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.63|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:85533e1a-a538-409f-8fb8-8ab901805386,timestamp:1628278928\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,491 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,491 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,491 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,491 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,595 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.61|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:ad088c4c-cb5d-425b-b541-17da01cbe511,timestamp:1628278928\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,596 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,596 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,596 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,596 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,597 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,689 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:2.13|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:1dd68ebe-f854-490c-8f09-bd9debc10847,timestamp:1628278928\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,690 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,690 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 5\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,690 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,690 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,691 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,783 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:2.11|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:0da726a5-544c-4782-858b-d16b84972d98,timestamp:1628278928\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,783 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,784 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 8\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,784 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,784 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,784 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,886 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,887 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.62|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:29408026-39b2-4372-89ea-555d64ca4ee8,timestamp:1628278928\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,887 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,887 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,887 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,887 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,998 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.47|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:654afa09-49d6-4c04-95dd-6fee49897799,timestamp:1628278928\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,998 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,998 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,998 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,999 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:08,999 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,090 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,090 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,090 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.54|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:0a6d5f97-9a1b-47e5-aefd-684a17193b7f,timestamp:1628278929\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,090 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,090 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,090 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,177 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,177 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:2.13|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:70c07758-7346-4781-9ba2-e5428b74ca36,timestamp:1628278929\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,178 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 5\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,178 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,179 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,179 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:5|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,274 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,274 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.64|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:840aa566-25d9-4b54-98e3-6e553fed5344,timestamp:1628278929\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,274 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,274 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,275 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,275 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,658 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,658 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:3.82|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:73745a81-2eb4-49b3-9b1d-357a56d0684b,timestamp:1628278929\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,658 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 5\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,658 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,659 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,659 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,800 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,800 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.79|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:67f51b4b-cb7f-48dd-995f-db5987d9395c,timestamp:1628278929\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,800 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,800 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,800 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,800 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,892 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,892 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.75|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:56d5d82a-4e92-4bbe-8e96-e88ed047bf25,timestamp:1628278929\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,893 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,893 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,893 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:09,893 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,482 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,482 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.62|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:18013dea-fbcb-4fdd-9f62-25f7695a7f76,timestamp:1628278930\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,483 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,483 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,483 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,484 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,606 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,606 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.62|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:b9e16c84-bb87-40bc-bc93-a07277ddb737,timestamp:1628278930\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,606 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,606 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,607 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,607 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,708 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,709 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.7|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:0d694505-395d-40c4-af6e-d3a2a28213ca,timestamp:1628278930\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,709 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,709 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,709 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,709 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,788 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,788 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.7|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:83841f59-1596-460a-8642-2ae2fb24d7c1,timestamp:1628278930\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,788 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,788 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,788 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,788 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,879 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,879 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.74|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:6bce3675-9ab7-4d5c-9411-2893e4a1ced0,timestamp:1628278930\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,880 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 4\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,880 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,880 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,880 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,951 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,952 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 5\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,951 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:2.06|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:332badac-c9dc-453f-b98e-8e1efc0c5108,timestamp:1628278930\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,952 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,952 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:10,952 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:11,034 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:11,034 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1.46|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:634e0007-b3ea-4b7a-8c14-c2c9e8789981,timestamp:1628278931\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:11,034 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:11,034 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:11,035 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:11,035 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:11,139 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:11,140 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:35938 \"POST /invocations HTTP/1.1\" 200 3\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:11,139 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:2.08|#ModelName:model,Level:Model|#hostname:562763372dec,requestID:9b71e504-fc7f-4032-bf1e-aed79c962c5e,timestamp:1628278931\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:11,140 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:11,140 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n",
      "\u001b[33m2021-08-06 19:42:11,140 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:562763372dec,timestamp:null\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dist_transformer = estimator.transformer(instance_count=2, instance_type=\"ml.c4.xlarge\")\n",
    "\n",
    "dist_transformer.transform(\n",
    "    data=inference_inputs,\n",
    "    data_type=\"S3Prefix\",\n",
    "    content_type=\"application/x-image\",\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927e97be",
   "metadata": {},
   "source": [
    "## Look at all transform jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e5da2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CreationTime': datetime.datetime(2021, 8, 6, 19, 36, 43, 78000, tzinfo=tzlocal()),\n",
      " 'LastModifiedTime': datetime.datetime(2021, 8, 6, 19, 42, 13, 292000, tzinfo=tzlocal()),\n",
      " 'TransformEndTime': datetime.datetime(2021, 8, 6, 19, 42, 13, 39000, tzinfo=tzlocal()),\n",
      " 'TransformJobArn': 'arn:aws:sagemaker:us-east-1:804604702169:transform-job/pytorch-training-2021-08-06-19-36-42-866',\n",
      " 'TransformJobName': 'pytorch-training-2021-08-06-19-36-42-866',\n",
      " 'TransformJobStatus': 'Completed'}\n",
      "{'CreationTime': datetime.datetime(2021, 8, 6, 19, 36, 24, 50000, tzinfo=tzlocal()),\n",
      " 'FailureReason': 'ClientError: See job logs for more information',\n",
      " 'LastModifiedTime': datetime.datetime(2021, 8, 6, 19, 41, 35, 534000, tzinfo=tzlocal()),\n",
      " 'TransformEndTime': datetime.datetime(2021, 8, 6, 19, 41, 25, 895000, tzinfo=tzlocal()),\n",
      " 'TransformJobArn': 'arn:aws:sagemaker:us-east-1:804604702169:transform-job/pytorch-inference-2021-08-06-19-36-23-811',\n",
      " 'TransformJobName': 'pytorch-inference-2021-08-06-19-36-23-811',\n",
      " 'TransformJobStatus': 'Failed'}\n",
      "{'CreationTime': datetime.datetime(2021, 8, 6, 19, 29, 48, 905000, tzinfo=tzlocal()),\n",
      " 'LastModifiedTime': datetime.datetime(2021, 8, 6, 19, 34, 58, 235000, tzinfo=tzlocal()),\n",
      " 'TransformEndTime': datetime.datetime(2021, 8, 6, 19, 34, 52, 741000, tzinfo=tzlocal()),\n",
      " 'TransformJobArn': 'arn:aws:sagemaker:us-east-1:804604702169:transform-job/pytorch-inference-2021-08-06-19-29-48-690',\n",
      " 'TransformJobName': 'pytorch-inference-2021-08-06-19-29-48-690',\n",
      " 'TransformJobStatus': 'Completed'}\n"
     ]
    }
   ],
   "source": [
    "tjs = sm_cli.list_transform_jobs()[\"TransformJobSummaries\"]\n",
    "for tj in tjs:\n",
    "    pp.pprint(tj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "595aed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CreationTime': datetime.datetime(2021, 8, 6, 19, 36, 43, 78000, tzinfo=tzlocal()),\n",
      " 'DataProcessing': {'InputFilter': '$',\n",
      "                    'JoinSource': 'None',\n",
      "                    'OutputFilter': '$'},\n",
      " 'Environment': {},\n",
      " 'ModelName': 'pytorch-training-2021-08-06-19-36-42-084',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '906',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Fri, 06 Aug 2021 19:45:22 GMT',\n",
      "                                      'x-amzn-requestid': 'a82ec13b-2039-4502-875e-c6ce8283af77'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': 'a82ec13b-2039-4502-875e-c6ce8283af77',\n",
      "                      'RetryAttempts': 0},\n",
      " 'TransformEndTime': datetime.datetime(2021, 8, 6, 19, 42, 13, 39000, tzinfo=tzlocal()),\n",
      " 'TransformInput': {'CompressionType': 'None',\n",
      "                    'ContentType': 'application/x-image',\n",
      "                    'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
      "                                                    'S3Uri': 's3://sagemaker-us-east-1-804604702169/batch_transform'}},\n",
      "                    'SplitType': 'None'},\n",
      " 'TransformJobArn': 'arn:aws:sagemaker:us-east-1:804604702169:transform-job/pytorch-training-2021-08-06-19-36-42-866',\n",
      " 'TransformJobName': 'pytorch-training-2021-08-06-19-36-42-866',\n",
      " 'TransformJobStatus': 'Completed',\n",
      " 'TransformOutput': {'AssembleWith': 'None',\n",
      "                     'KmsKeyId': '',\n",
      "                     'S3OutputPath': 's3://sagemaker-us-east-1-804604702169/pytorch-training-2021-08-06-19-36-42-866'},\n",
      " 'TransformResources': {'InstanceCount': 2, 'InstanceType': 'ml.c4.xlarge'},\n",
      " 'TransformStartTime': datetime.datetime(2021, 8, 6, 19, 40, 27, 100000, tzinfo=tzlocal())}\n"
     ]
    }
   ],
   "source": [
    "import pprint as pp\n",
    "\n",
    "res = sm_cli.describe_transform_job(TransformJobName=dist_transformer.latest_transform_job.name)\n",
    "\n",
    "pp.pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15dd0889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-804604702169 pytorch-training-2021-08-06-19-36-42-866\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def get_bucket_and_prefix(s3_output_path):\n",
    "    trim = re.sub(\"s3://\", \"\", s3_output_path)\n",
    "    bucket, prefix = trim.split(\"/\")\n",
    "    return bucket, prefix\n",
    "\n",
    "\n",
    "local_path = \"output\"  # where to save the output locally\n",
    "\n",
    "bucket, output_prefix = get_bucket_and_prefix(res[\"TransformOutput\"][\"S3OutputPath\"])\n",
    "print(bucket, output_prefix)\n",
    "\n",
    "sagemaker_session.download_data(path=local_path, bucket=bucket, key_prefix=output_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00c53205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.png.out   25.png.out\t40.png.out  56.png.out\t71.png.out  87.png.out\r\n",
      "10.png.out  26.png.out\t41.png.out  57.png.out\t72.png.out  88.png.out\r\n",
      "11.png.out  27.png.out\t42.png.out  58.png.out\t73.png.out  89.png.out\r\n",
      "12.png.out  28.png.out\t43.png.out  59.png.out\t74.png.out  8.png.out\r\n",
      "13.png.out  29.png.out\t44.png.out  5.png.out\t75.png.out  90.png.out\r\n",
      "14.png.out  2.png.out\t45.png.out  60.png.out\t76.png.out  91.png.out\r\n",
      "15.png.out  30.png.out\t46.png.out  61.png.out\t77.png.out  92.png.out\r\n",
      "16.png.out  31.png.out\t47.png.out  62.png.out\t78.png.out  93.png.out\r\n",
      "17.png.out  32.png.out\t48.png.out  63.png.out\t79.png.out  94.png.out\r\n",
      "18.png.out  33.png.out\t49.png.out  64.png.out\t7.png.out   95.png.out\r\n",
      "19.png.out  34.png.out\t4.png.out   65.png.out\t80.png.out  96.png.out\r\n",
      "1.png.out   35.png.out\t50.png.out  66.png.out\t81.png.out  97.png.out\r\n",
      "20.png.out  36.png.out\t51.png.out  67.png.out\t82.png.out  98.png.out\r\n",
      "21.png.out  37.png.out\t52.png.out  68.png.out\t83.png.out  99.png.out\r\n",
      "22.png.out  38.png.out\t53.png.out  69.png.out\t84.png.out  9.png.out\r\n",
      "23.png.out  39.png.out\t54.png.out  6.png.out\t85.png.out\r\n",
      "24.png.out  3.png.out\t55.png.out  70.png.out\t86.png.out\r\n"
     ]
    }
   ],
   "source": [
    "!ls {local_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88adc8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': 6}\n",
      "{'predictions': 2}\n",
      "{'predictions': 8}\n",
      "{'predictions': 9}\n",
      "{'predictions': 4}\n",
      "{'predictions': 9}\n",
      "{'predictions': 7}\n",
      "{'predictions': 4}\n",
      "{'predictions': 1}\n",
      "{'predictions': 4}\n",
      "{'predictions': 9}\n",
      "{'predictions': 0}\n",
      "{'predictions': 2}\n",
      "{'predictions': 7}\n",
      "{'predictions': 5}\n",
      "{'predictions': 7}\n",
      "{'predictions': 2}\n",
      "{'predictions': 0}\n",
      "{'predictions': 2}\n",
      "{'predictions': 4}\n",
      "{'predictions': 8}\n",
      "{'predictions': 1}\n",
      "{'predictions': 5}\n",
      "{'predictions': 8}\n",
      "{'predictions': 9}\n",
      "{'predictions': 4}\n",
      "{'predictions': 4}\n",
      "{'predictions': 6}\n",
      "{'predictions': 4}\n",
      "{'predictions': 2}\n",
      "{'predictions': 7}\n",
      "{'predictions': 7}\n",
      "{'predictions': 6}\n",
      "{'predictions': 1}\n",
      "{'predictions': 1}\n",
      "{'predictions': 4}\n",
      "{'predictions': 4}\n",
      "{'predictions': 0}\n",
      "{'predictions': 6}\n",
      "{'predictions': 0}\n",
      "{'predictions': 5}\n",
      "{'predictions': 5}\n",
      "{'predictions': 4}\n",
      "{'predictions': 4}\n",
      "{'predictions': 4}\n",
      "{'predictions': 4}\n",
      "{'predictions': 1}\n",
      "{'predictions': 3}\n",
      "{'predictions': 9}\n",
      "{'predictions': 5}\n",
      "{'predictions': 4}\n",
      "{'predictions': 6}\n",
      "{'predictions': 3}\n",
      "{'predictions': 1}\n",
      "{'predictions': 5}\n",
      "{'predictions': 3}\n",
      "{'predictions': 4}\n",
      "{'predictions': 9}\n",
      "{'predictions': 2}\n",
      "{'predictions': 6}\n",
      "{'predictions': 5}\n",
      "{'predictions': 7}\n",
      "{'predictions': 0}\n",
      "{'predictions': 6}\n",
      "{'predictions': 8}\n",
      "{'predictions': 7}\n",
      "{'predictions': 8}\n",
      "{'predictions': 6}\n",
      "{'predictions': 2}\n",
      "{'predictions': 7}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 1}\n",
      "{'predictions': 4}\n",
      "{'predictions': 0}\n",
      "{'predictions': 9}\n",
      "{'predictions': 7}\n",
      "{'predictions': 6}\n",
      "{'predictions': 4}\n",
      "{'predictions': 7}\n",
      "{'predictions': 7}\n",
      "{'predictions': 3}\n",
      "{'predictions': 0}\n",
      "{'predictions': 3}\n",
      "{'predictions': 1}\n",
      "{'predictions': 8}\n",
      "{'predictions': 9}\n",
      "{'predictions': 1}\n",
      "{'predictions': 1}\n",
      "{'predictions': 4}\n",
      "{'predictions': 0}\n",
      "{'predictions': 3}\n",
      "{'predictions': 4}\n",
      "{'predictions': 7}\n",
      "{'predictions': 3}\n",
      "{'predictions': 0}\n",
      "{'predictions': 9}\n",
      "{'predictions': 4}\n",
      "{'predictions': 3}\n",
      "{'predictions': 5}\n"
     ]
    }
   ],
   "source": [
    "# Inspect the output\n",
    "\n",
    "import json\n",
    "\n",
    "for f in os.listdir(local_path):\n",
    "    path = os.path.join(local_path, f)\n",
    "    with open(path, \"r\") as f:\n",
    "        pred = json.load(f)\n",
    "        print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 333.854918,
   "end_time": "2021-06-03T00:15:43.072184",
   "environment_variables": {},
   "exception": true,
   "input_path": "pytorch-mnist-batch-transform.ipynb",
   "output_path": "/opt/ml/processing/output/pytorch-mnist-batch-transform-2021-06-03-00-06-06.ipynb",
   "parameters": {
    "kms_key": "arn:aws:kms:us-west-2:521695447989:key/6e9984db-50cf-4c7e-926c-877ec47a8b25"
   },
   "start_time": "2021-06-03T00:10:09.217266",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01005530a5b1473b9f4a024b19c04c0e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_968ed82ad8f0453e8f81a839df4428db",
       "placeholder": "​",
       "style": "IPY_MODEL_e4f0965e53ee40adb1ae44da87428325",
       "value": "  0%"
      }
     },
     "0995f6633c0f4facabe6759837c606ba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "1410dcfcd117434889e9594cdde4e1b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "18caaab41d6146c1824859691f6cb435": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d823500ff0dc4c2198b83cd231f8bffe",
       "placeholder": "​",
       "style": "IPY_MODEL_7dab31892241494e8d27d38ca98e5aa6",
       "value": " 0/28881 [00:00&lt;?, ?it/s]"
      }
     },
     "19ef65b0ecae45bdbca066cea679878d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2ceacd43f28744eb9b7a12f8276b6016",
        "IPY_MODEL_e44ddce6c5704f0b9495ee662806f5f6",
        "IPY_MODEL_7717cc87ebcc4c0581ae32848b40982c"
       ],
       "layout": "IPY_MODEL_59d0678977a343abb8a02dc5c9699b89"
      }
     },
     "2126024805384bff9b0409b4dc91e60c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "216ba33f9f1b486ebac2a6fce0510246": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f94b5a0d68c541e894e325a0e2f899d2",
       "placeholder": "​",
       "style": "IPY_MODEL_633cc1cdb94e43a6a07559483496c60d",
       "value": "  0%"
      }
     },
     "23445154eb524df985b5a755fcbddd32": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_216ba33f9f1b486ebac2a6fce0510246",
        "IPY_MODEL_c4f4f4bfe979469c9bc59ab73bbf518f",
        "IPY_MODEL_fe83e178358040eaa07f6198ba693fc9"
       ],
       "layout": "IPY_MODEL_cf1f337300394948bce741af7bcd8b8c"
      }
     },
     "235ae38cf16e4aacb95c3d16d9749da3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2c2474d5a8144bf8930fa5cc02c73ccf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5495428879544d6da73e2ed7e70f0c96",
        "IPY_MODEL_6e2a4641cd944d9a8196f4a836e90590",
        "IPY_MODEL_9179e5f467c8450a988b988d7da06090"
       ],
       "layout": "IPY_MODEL_596f8cbad0884ec79cf6ee757cc9f38a"
      }
     },
     "2ceacd43f28744eb9b7a12f8276b6016": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a540362f86774590851c1d0892bea723",
       "placeholder": "​",
       "style": "IPY_MODEL_bb9ebd025f05499da7b847b8ef7a9ff5",
       "value": ""
      }
     },
     "495839f4239743669d9ee61cfbc33967": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4d62b9fde9104c8081b545c3933a077e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "51a28ca59cf9407ea0e02da868d79ebd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5495428879544d6da73e2ed7e70f0c96": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_235ae38cf16e4aacb95c3d16d9749da3",
       "placeholder": "​",
       "style": "IPY_MODEL_fe60ae53dd1646ca91018ba20934948b",
       "value": ""
      }
     },
     "596f8cbad0884ec79cf6ee757cc9f38a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "59d0678977a343abb8a02dc5c9699b89": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "633cc1cdb94e43a6a07559483496c60d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "63a57f663bfa4a1585c1ba36501b6b23": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6e2a4641cd944d9a8196f4a836e90590": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "info",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fb8c653eeeb24799bcc9279389fdb523",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b513a456776d40b496f035c64360db90",
       "value": 1
      }
     },
     "7717cc87ebcc4c0581ae32848b40982c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9b63360561b34257b171498e67902dda",
       "placeholder": "​",
       "style": "IPY_MODEL_f86487d9a78940a394503b2bea77d756",
       "value": " 9920512/? [04:50&lt;00:00, 36552.15it/s]"
      }
     },
     "7bceed60fb344aa182dccc3dcf0ee886": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_01005530a5b1473b9f4a024b19c04c0e",
        "IPY_MODEL_e82a5227430443d98d29555fd77b2bd3",
        "IPY_MODEL_18caaab41d6146c1824859691f6cb435"
       ],
       "layout": "IPY_MODEL_63a57f663bfa4a1585c1ba36501b6b23"
      }
     },
     "7dab31892241494e8d27d38ca98e5aa6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8b5b76e77cb14ecf95a310ba46ed86f5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "9179e5f467c8450a988b988d7da06090": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_94ef992b73d44d829b863815da70111f",
       "placeholder": "​",
       "style": "IPY_MODEL_1410dcfcd117434889e9594cdde4e1b0",
       "value": " 1654784/? [00:47&lt;00:00, 33514.08it/s]"
      }
     },
     "94ef992b73d44d829b863815da70111f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "968ed82ad8f0453e8f81a839df4428db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9b63360561b34257b171498e67902dda": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a540362f86774590851c1d0892bea723": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b513a456776d40b496f035c64360db90": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "bb9ebd025f05499da7b847b8ef7a9ff5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c0b88a223b374693b6b0c74db9ffe346": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "c4f4f4bfe979469c9bc59ab73bbf518f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "info",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8b5b76e77cb14ecf95a310ba46ed86f5",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_51a28ca59cf9407ea0e02da868d79ebd",
       "value": 0
      }
     },
     "cf1f337300394948bce741af7bcd8b8c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d823500ff0dc4c2198b83cd231f8bffe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e44ddce6c5704f0b9495ee662806f5f6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "info",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0995f6633c0f4facabe6759837c606ba",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4d62b9fde9104c8081b545c3933a077e",
       "value": 1
      }
     },
     "e4f0965e53ee40adb1ae44da87428325": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e82a5227430443d98d29555fd77b2bd3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "info",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c0b88a223b374693b6b0c74db9ffe346",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_495839f4239743669d9ee61cfbc33967",
       "value": 0
      }
     },
     "eb4c77cfe2c54976aef8efc0e3207140": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f86487d9a78940a394503b2bea77d756": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f94b5a0d68c541e894e325a0e2f899d2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fb8c653eeeb24799bcc9279389fdb523": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "fe60ae53dd1646ca91018ba20934948b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fe83e178358040eaa07f6198ba693fc9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2126024805384bff9b0409b4dc91e60c",
       "placeholder": "​",
       "style": "IPY_MODEL_eb4c77cfe2c54976aef8efc0e3207140",
       "value": " 0/4542 [00:00&lt;?, ?it/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
